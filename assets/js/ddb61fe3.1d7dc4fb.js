"use strict";(self.webpackChunkcluster_factory_ce_docs=self.webpackChunkcluster_factory_ce_docs||[]).push([[9101],{9613:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var l=n(9496);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);t&&(l=l.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,l)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,l,a=function(e,t){if(null==e)return{};var n,l,a={},r=Object.keys(e);for(l=0;l<r.length;l++)n=r[l],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(l=0;l<r.length;l++)n=r[l],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var i=l.createContext({}),u=function(e){var t=l.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},c=function(e){var t=u(e.components);return l.createElement(i.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return l.createElement(l.Fragment,{},t)}},p=l.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,i=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),p=u(n),d=a,g=p["".concat(i,".").concat(d)]||p[d]||m[d]||r;return n?l.createElement(g,s(s({ref:t},c),{},{components:n})):l.createElement(g,s({ref:t},c))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,s=new Array(r);s[0]=p;var o={};for(var i in t)hasOwnProperty.call(t,i)&&(o[i]=t[i]);o.originalType=e,o.mdxType="string"==typeof e?e:a,s[1]=o;for(var u=2;u<r;u++)s[u]=n[u];return l.createElement.apply(null,s)}return l.createElement.apply(null,n)}p.displayName="MDXCreateElement"},8751:(e,t,n)=>{n.d(t,{Z:()=>s});var l=n(9496),a=n(5924);const r="tabItem_cuvL";function s(e){let{children:t,hidden:n,className:s}=e;return l.createElement("div",{role:"tabpanel",className:(0,a.Z)(r,s),hidden:n},t)}},5632:(e,t,n)=>{n.d(t,{Z:()=>d});var l=n(665),a=n(9496),r=n(5924),s=n(2981),o=n(8291),i=n(9825),u=n(3772);const c="tabList_vL6X",m="tabItem_MPSI";function p(e){var t;const{lazy:n,block:s,defaultValue:p,values:d,groupId:g,className:h}=e,y=a.Children.map(e.children,(e=>{if((0,a.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),k=d??y.map((e=>{let{props:{value:t,label:n,attributes:l}}=e;return{value:t,label:n,attributes:l}})),b=(0,o.l)(k,((e,t)=>e.value===t.value));if(b.length>0)throw new Error(`Docusaurus error: Duplicate values "${b.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const f=null===p?p:p??(null==(t=y.find((e=>e.props.default)))?void 0:t.props.value)??y[0].props.value;if(null!==f&&!k.some((e=>e.value===f)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${f}" but none of its children has the corresponding value. Available values are: ${k.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:v,setTabGroupChoices:N}=(0,i.U)(),[S,C]=(0,a.useState)(f),L=[],{blockElementScrollPositionUntilNextRender:_}=(0,u.o5)();if(null!=g){const e=v[g];null!=e&&e!==S&&k.some((t=>t.value===e))&&C(e)}const E=e=>{const t=e.currentTarget,n=L.indexOf(t),l=k[n].value;l!==S&&(_(t),C(l),null!=g&&N(g,String(l)))},w=e=>{var t;let n=null;switch(e.key){case"Enter":E(e);break;case"ArrowRight":{const t=L.indexOf(e.currentTarget)+1;n=L[t]??L[0];break}case"ArrowLeft":{const t=L.indexOf(e.currentTarget)-1;n=L[t]??L[L.length-1];break}}null==(t=n)||t.focus()};return a.createElement("div",{className:(0,r.Z)("tabs-container",c)},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":s},h)},k.map((e=>{let{value:t,label:n,attributes:s}=e;return a.createElement("li",(0,l.Z)({role:"tab",tabIndex:S===t?0:-1,"aria-selected":S===t,key:t,ref:e=>L.push(e),onKeyDown:w,onClick:E},s,{className:(0,r.Z)("tabs__item",m,null==s?void 0:s.className,{"tabs__item--active":S===t})}),n??t)}))),n?(0,a.cloneElement)(y.filter((e=>e.props.value===S))[0],{className:"margin-top--md"}):a.createElement("div",{className:"margin-top--md"},y.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==S})))))}function d(e){const t=(0,s.Z)();return a.createElement(p,(0,l.Z)({key:String(t)},e))}},7533:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>u,toc:()=>m});var l=n(665),a=(n(9496),n(9613)),r=n(5632),s=n(8751);const o={},i="Deploying SLURM Cluster",u={unversionedId:"guides/slurm/deploy-slurm",id:"guides/slurm/deploy-slurm",title:"Deploying SLURM Cluster",description:"image-20220512151655613",source:"@site/docs/guides/60-slurm/01-deploy-slurm.mdx",sourceDirName:"guides/60-slurm",slug:"/guides/slurm/deploy-slurm",permalink:"/docs/guides/slurm/deploy-slurm",draft:!1,editUrl:"https://github.com/SquareFactory/ClusterFactory/tree/main/web/docs/guides/60-slurm/01-deploy-slurm.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"docs",previous:{title:"GitOps with xCAT",permalink:"/docs/guides/provisioning/gitops-with-xcat"},next:{title:"Deploy Open OnDemand",permalink:"/docs/guides/slurm/deploy-ondemand"}},c={},m=[{value:"Helm and Docker resources",id:"helm-and-docker-resources",level:2},{value:"1. Preparation",id:"1-preparation",level:2},{value:"Namespace and AppProject",id:"namespace-and-appproject",level:3},{value:"LDAP deployment",id:"ldap-deployment",level:3},{value:"SSSD configuration",id:"sssd-configuration",level:3},{value:"MySQL deployment",id:"mysql-deployment",level:3},{value:"JWT Key generation",id:"jwt-key-generation",level:3},{value:"MUNGE Key generation",id:"munge-key-generation",level:3},{value:"2. Begin writing the <code>slurm-cluster-&lt;cluster name&gt;-app.yaml</code>",id:"2-begin-writing-the-slurm-cluster-cluster-name-appyaml",level:2},{value:"2.a. Argo CD Application configuration",id:"2a-argo-cd-application-configuration",level:3},{value:"2.b. Values: Configuring the SLURM cluster",id:"2b-values-configuring-the-slurm-cluster",level:3},{value:"3. Slurm DB Deployment",id:"3-slurm-db-deployment",level:2},{value:"3.a. Secrets",id:"3a-secrets",level:3},{value:"3.b. Values: Enable SLURM DB",id:"3b-values-enable-slurm-db",level:3},{value:"4. Slurm Controller Deployment",id:"4-slurm-controller-deployment",level:2},{value:"4.a. Volumes",id:"4a-volumes",level:3},{value:"4.b. Values: Enable SLURM Controller",id:"4b-values-enable-slurm-controller",level:3},{value:"4.c Testing: <code>sinfo</code> from the controller node",id:"4c-testing-sinfo-from-the-controller-node",level:3},{value:"5. Slurm Compute Bare-Metal Deployment",id:"5-slurm-compute-bare-metal-deployment",level:2},{value:"5.a. Build an OS Image with Slurm",id:"5a-build-an-os-image-with-slurm",level:3},{value:"5.b. xCAT Postbootscripts",id:"5b-xcat-postbootscripts",level:3},{value:"5.c. Reboot the nodes",id:"5c-reboot-the-nodes",level:3},{value:"6. Slurm Login Deployment",id:"6-slurm-login-deployment",level:2},{value:"6.a. Secrets and Volumes",id:"6a-secrets-and-volumes",level:3},{value:"SSH Server configuration",id:"ssh-server-configuration",level:4},{value:"Home directory for the LDAP users",id:"home-directory-for-the-ldap-users",level:4},{value:"6.b. Values: Enable SLURM Login",id:"6b-values-enable-slurm-login",level:3},{value:"6.c Testing: Access to a SLURM Login node",id:"6c-testing-access-to-a-slurm-login-node",level:3}],p={toc:m};function d(e){let{components:t,...o}=e;return(0,a.kt)("wrapper",(0,l.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"deploying-slurm-cluster"},"Deploying SLURM Cluster"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"image-20220512151655613",src:n(3210).Z,width:"1065",height:"540"})),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"Deploying the SLURM database isn't stable yet. Please feel free to ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/SquareFactory/ClusterFactory/issues/new"},"create an issue")," so we can improve its stability.")),(0,a.kt)("h2",{id:"helm-and-docker-resources"},"Helm and Docker resources"),(0,a.kt)("p",null,"The Helm resources are stored on ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/SquareFactory/ClusterFactory/tree/main/helm/slurm-cluster"},"the ClusterFactory Git Repository"),"."),(0,a.kt)("p",null,"The Dockerfile is described in the git repository ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/SquareFactory/slurm-docker"},"SquareFactory/slurm-docker"),"."),(0,a.kt)("p",null,"The Docker images can be pulled with:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},"docker pull ghcr.io/squarefactory/slurm:latest-controller\ndocker pull ghcr.io/squarefactory/slurm:latest-login\ndocker pull ghcr.io/squarefactory/slurm:latest-db\ndocker pull ghcr.io/squarefactory/slurm:latest-rest\n")),(0,a.kt)("admonition",{type:"note"},(0,a.kt)("p",{parentName:"admonition"},"You should always verify the default Helm ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/SquareFactory/ClusterFactory/blob/main/helm/slurm-cluster/values.yaml"},"values")," before editing the ",(0,a.kt)("inlineCode",{parentName:"p"},"values")," field of an Argo CD ",(0,a.kt)("inlineCode",{parentName:"p"},"Application"),".")),(0,a.kt)("h2",{id:"1-preparation"},"1. Preparation"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Compared to the other guides we will start from scratch.")),(0,a.kt)("p",null,"Delete the ",(0,a.kt)("inlineCode",{parentName:"p"},"argo/slurm-cluster")," directory (or rename it)."),(0,a.kt)("p",null,"Deploying a SLURM cluster isn't easy and you MUST have these components ready:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"A LDAP server and a SSSD configuration, to synchronize the user ID across the cluster"),(0,a.kt)("li",{parentName:"ul"},"A MySQL server for the SLURM DB"),(0,a.kt)("li",{parentName:"ul"},"A JWT private key, for the authentication via REST API"),(0,a.kt)("li",{parentName:"ul"},"A MUNGE key, for the authentication of SLURM daemons")),(0,a.kt)("h3",{id:"namespace-and-appproject"},"Namespace and AppProject"),(0,a.kt)("p",null,"Create and apply the ",(0,a.kt)("inlineCode",{parentName:"p"},"Namespace")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"AppProject"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/namespace.yaml"',title:'"argo/slurm-cluster/namespace.yaml"'},"apiVersion: v1\nkind: Namespace\nmetadata:\n  name: slurm-cluster\n  labels:\n    app.kubernetes.io/name: slurm-cluster\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/app-project.yaml"',title:'"argo/slurm-cluster/app-project.yaml"'},"apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: slurm-cluster\n  namespace: argocd\n  # Finalizer that ensures that project is not deleted until it is not referenced by any application\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  description: Slurm cluster\n  # Allow manifests to deploy from any Git repos\n  sourceRepos:\n    - '*'\n  # Only permit applications to deploy to the namespace in the same cluster\n  destinations:\n    - namespace: slurm-cluster\n      server: https://kubernetes.default.svc\n\n  namespaceResourceWhitelist:\n    - kind: '*'\n      group: '*'\n\n  clusterResourceWhitelist:\n    - kind: '*'\n      group: '*'\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/\n")),(0,a.kt)("h3",{id:"ldap-deployment"},"LDAP deployment"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"Follow ",(0,a.kt)("a",{parentName:"p",href:"/docs/guides/deploy-ldap"},"the guide"),".")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"Open a shell on the LDAP server container and create a user and a group ",(0,a.kt)("inlineCode",{parentName:"p"},"slurm"),":"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="Pod: dirsrv-389ds-0 (namespace: ldap)"',title:'"Pod:',"dirsrv-389ds-0":!0,"(namespace:":!0,'ldap)"':!0},'# Create user\ndsidm -b "dc=example,dc=com" localhost user create \\\n  --uid slurm \\\n  --cn slurm \\\n  --displayName slurm \\\n  --homeDirectory "/dev/shm" \\\n  --uidNumber 1501 \\\n  --gidNumber 1501\n\n# Create group\ndsidm -b "dc=example,dc=com" localhost group create \\\n  --cn slurm\n\n# Add posixGroup property and gidNumber\ndsidm -b "dc=example,dc=com" localhost group modify slurm \\\n  "add:objectClass:posixGroup" \\\n  "add:gidNumber:1501"\n')),(0,a.kt)("h3",{id:"sssd-configuration"},"SSSD configuration"),(0,a.kt)("p",null,"Let's store it in a ",(0,a.kt)("inlineCode",{parentName:"p"},"Secret"),":"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Create the ",(0,a.kt)("inlineCode",{parentName:"li"},"argo/slurm-cluster/secrets/")," directory and create a ",(0,a.kt)("inlineCode",{parentName:"li"},"-secret.yaml.local")," file:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/sssd-secret.yaml.local"',title:'"argo/slurm-cluster/secrets/sssd-secret.yaml.local"'},'apiVersion: v1\nkind: Secret\nmetadata:\n  name: sssd-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  sssd.conf: |\n    # https://sssd.io/docs/users/troubleshooting/how-to-troubleshoot-backend.html\n    [sssd]\n    services = nss,pam,sudo,ssh\n    config_file_version = 2\n    domains = example-ldap\n\n    [sudo]\n\n    [nss]\n\n    [pam]\n    offline_credentials_expiration = 60\n\n    [domain/example-ldap]\n    debug_level=3\n    cache_credentials = True\n    dns_resolver_timeout = 15\n\n    override_homedir = /home/ldap-users/%u\n\n    id_provider = ldap\n    auth_provider = ldap\n    access_provider = ldap\n    chpass_provider = ldap\n\n    ldap_schema = rfc2307bis\n\n    ldap_uri = ldaps://dirsrv-389ds.ldap.svc.cluster.local:3636\n    ldap_default_bind_dn = cn=Directory Manager\n    ldap_default_authtok = <password>\n    ldap_search_timeout = 50\n    ldap_network_timeout = 60\n    ldap_user_member_of = memberof\n    ldap_user_gecos = cn\n    ldap_user_uuid = nsUniqueId\n    ldap_group_uuid = nsUniqueId\n\n    ldap_search_base = ou=people,dc=example,dc=com\n    ldap_group_search_base = ou=groups,dc=example,dc=com\n    ldap_sudo_search_base = ou=sudoers,dc=example,dc=com\n    ldap_user_ssh_public_key = nsSshPublicKey\n\n    ldap_account_expire_policy = rhds\n    ldap_access_order = filter, expire\n    ldap_access_filter = (objectClass=posixAccount)\n\n    ldap_tls_cipher_suite = HIGH\n    # On Ubuntu, the LDAP client is linked to GnuTLS instead of OpenSSL => cipher suite names are different\n    # In fact, it\'s not even a cipher suite name that goes here, but a so called "priority list" => see $> gnutls-cli --priority-list\n    # See https://backreference.org/2009/11/18/openssl-vs-gnutls-cipher-names/ , gnutls-cli is part of package gnutls-bin\n')),(0,a.kt)("p",null,"Adapt this secret based on your LDAP configuration."),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},"Seal the secret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"cfctl kubeseal\n")),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},"Apply the SealedSecret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/secrets/sssd-sealed-secret.yaml\n")),(0,a.kt)("h3",{id:"mysql-deployment"},"MySQL deployment"),(0,a.kt)("p",null,"You can deploy MySQL using the ",(0,a.kt)("a",{parentName:"p",href:"https://bitnami.com/stack/mysql/helm"},"Helm Chart of Bitnami")," and develop an ",(0,a.kt)("a",{parentName:"p",href:"/docs/guides/develop-apps-for-cluster-factory"},"Argo CD app"),"."),(0,a.kt)("p",null,"After deploying the MySQL/MariaDB server, you must create a slurm database. Open a shell on the MySQL container and run:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="Pod: mariadb-0"',title:'"Pod:','mariadb-0"':!0},"mysql -u root -p -h localhost\n# Enter your root password\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="Pod: mariadb-0 (sql)"',title:'"Pod:',"mariadb-0":!0,'(sql)"':!0},"create user 'slurm'@'%' identified by '<your password>';\ngrant all on slurm_acct_db.* TO 'slurm'@'%';\ncreate database slurm_acct_db;\n")),(0,a.kt)("h3",{id:"jwt-key-generation"},"JWT Key generation"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local"',title:'"user@local"'},"ssh-keygen -t rsa -b 4096 -m PEM -f jwtRS256.key\n")),(0,a.kt)("p",null,"Let's store it in a ",(0,a.kt)("inlineCode",{parentName:"p"},"Secret"),":"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Create a ",(0,a.kt)("inlineCode",{parentName:"li"},"-secret.yaml.local")," file:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/slurm-secret.yaml.local"',title:'"argo/slurm-cluster/secrets/slurm-secret.yaml.local"'},"apiVersion: v1\nkind: Secret\nmetadata:\n  name: slurm-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  jwt_hs256.key: |\n    -----BEGIN RSA PRIVATE KEY-----\n    ...\n    -----END RSA PRIVATE KEY-----\n")),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},"Seal the secret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"cfctl kubeseal\n")),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},"Apply the SealedSecret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/secrets/slurm-sealed-secret.yaml\n")),(0,a.kt)("h3",{id:"munge-key-generation"},"MUNGE Key generation"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="root@local"',title:'"root@local"'},"# As root\ndnf install -y munge\n/usr/sbin/create-munge-key\ncat /etc/munge/munge.key | base64\n")),(0,a.kt)("p",null,"Let's store it in a ",(0,a.kt)("inlineCode",{parentName:"p"},"Secret"),":"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Create a ",(0,a.kt)("inlineCode",{parentName:"li"},"-secret.yaml.local")," file:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/munge-secret.yaml.local"',title:'"argo/slurm-cluster/secrets/munge-secret.yaml.local"'},"apiVersion: v1\nkind: Secret\nmetadata:\n  name: munge-secret\n  namespace: slurm-cluster\ntype: Opaque\ndata:\n  munge.key: |\n    <base 64 encoded key>\n")),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},"Seal the secret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"cfctl kubeseal\n")),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},"Apply the SealedSecret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/cvmfs/secrets/munge-sealed-secret.yaml\n")),(0,a.kt)("h2",{id:"2-begin-writing-the-slurm-cluster-cluster-name-appyaml"},"2. Begin writing the ",(0,a.kt)("inlineCode",{parentName:"h2"},"slurm-cluster-<cluster name>-app.yaml")),(0,a.kt)("h3",{id:"2a-argo-cd-application-configuration"},"2.a. Argo CD Application configuration"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml"',title:'"argo/slurm-cluster/apps/slurm-cluster-<cluster','name>-app.yaml"':!0},'apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: slurm-cluster-<FILL ME: cluster name>-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: slurm-cluster\n  source:\n   # You should have forked this repo. Change the URL to your fork.\n    repoURL: git@github.com:<FILL ME: your account>/ClusterFactory.git\n    # You should use your branch too.\n    targetRevision: HEAD\n    path: helm/slurm-cluster\n    helm:\n      releaseName: slurm-cluster-<FILL ME: cluster name>\n\n      valueFiles:\n        - values-<FILL ME: cluster name>.yaml\n\n  destination:\n    server: \'https://kubernetes.default.svc\'\n    namespace: slurm-cluster\n\n  syncPolicy:\n    automated:\n      prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\n      selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\n      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).\n    syncOptions: []\n    retry:\n      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0\n      backoff:\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")\n        factor: 2 # a factor to multiply the base duration after each failed retry\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\n')),(0,a.kt)("h3",{id:"2b-values-configuring-the-slurm-cluster"},"2.b. Values: Configuring the SLURM cluster"),(0,a.kt)("p",null,"Add:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},'sssd:\n  secretName: sssd-secret\n\nmunge:\n  secretName: munge-secret\n\njwt:\n  secretName: slurm-secret\n\nslurmConfig:\n  clusterName: <FILL ME: cluster-name>\n\n  compute:\n    srunPortRangeStart: 60001\n    srunPortRangeEnd: 63000\n    debug: debug5\n\n  accounting: |\n    AccountingStorageType=accounting_storage/slurmdbd\n    AccountingStorageHost=slurm-cluster-<FILL ME: cluster name>.slurm-cluster.svc.cluster.local\n    AccountingStoragePort=6819\n    AccountingStorageTRES=gres/gpu\n\n  controller:\n    parameters: enable_configless\n    debug: debug5\n\n  defaultResourcesAllocation: |\n    # Change accordingly\n    DefCpuPerGPU=4\n    DefMemPerCpu=7000\n\n  nodes: |\n    # Change accordingly\n    NodeName=cn[1-12]  CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128473 Gres=gpu:4\n\n  partitions: |\n    # Change accordingly\n    PartitionName=main Nodes=cn[1-12] Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"\n\n  gres: |\n    # Change accordingly\n    NodeName=cn[1-12] File=/dev/nvidia[0-3] AutoDetect=nvml\n\n  # Extra slurm.conf configuration\n  extra: |\n    LaunchParameters=enable_nss_slurm\n    DebugFlags=Script,Gang,SelectType\n    TCPTimeout=5\n\n    # MPI stacks running over Infiniband or OmniPath require the ability to allocate more\n    # locked memory than the default limit. Unfortunately, user processes on login nodes\n    # may have a small memory limit (check it by ulimit -a) which by default are propagated\n    # into Slurm jobs and hence cause fabric errors for MPI.\n    PropagateResourceLimitsExcept=MEMLOCK\n\n    ProctrackType=proctrack/cgroup\n    TaskPlugin=task/cgroup\n    SwitchType=switch/none\n    MpiDefault=pmix_v2\n    ReturnToService=2\n    GresTypes=gpu\n    PreemptType=preempt/qos\n    PreemptMode=REQUEUE\n    PreemptExemptTime=-1\n    Prolog=/etc/slurm/prolog.d/*\n    Epilog=/etc/slurm/epilog.d/*\n\n    # Federation\n    FederationParameters=fed_display\n')),(0,a.kt)("h2",{id:"3-slurm-db-deployment"},"3. Slurm DB Deployment"),(0,a.kt)("h3",{id:"3a-secrets"},"3.a. Secrets"),(0,a.kt)("p",null,"Assuming you have deployed LDAP and MySQL, we will store the ",(0,a.kt)("inlineCode",{parentName:"p"},"slurmdbd.conf")," inside a secret:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Create a ",(0,a.kt)("inlineCode",{parentName:"li"},"-secret.yaml.local")," file:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/slurmdbd-conf-secret.yaml.local"',title:'"argo/slurm-cluster/secrets/slurmdbd-conf-secret.yaml.local"'},"apiVersion: v1\nkind: Secret\nmetadata:\n  name: slurmdbd-conf-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  slurmdbd.conf: |\n    # See https://slurm.schedmd.com/slurmdbd.conf.html\n    ### Main\n    DbdHost=slurm-cluster-<FILL ME: cluster name>-db-0\n    SlurmUser=slurm\n\n    ### Logging\n    DebugLevel=debug5   # optional, defaults to 'info'. Possible values: fatal, error, info, verbose, debug, debug[2-5]\n    LogFile=/var/log/slurm/slurmdbd.log\n    PidFile=/var/run/slurmdbd.pid\n    LogTimeFormat=thread_id\n\n    AuthAltTypes=auth/jwt\n    AuthAltParameters=jwt_key=/var/spool/slurm/jwt_hs256.key\n\n    ### Database server configuration\n    StorageType=accounting_storage/mysql\n    StorageHost=<FILL ME>\n    StorageUser=<FILL ME>\n    StoragePass=<FILL ME>\n")),(0,a.kt)("p",null,"Replace the ",(0,a.kt)("inlineCode",{parentName:"p"},"<FILL ME>")," according to your existing configuration."),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},"Seal the secret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"cfctl kubeseal\n")),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},"Apply the SealedSecret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/secrets/slurmdbd-conf-sealed-secret.yaml\n")),(0,a.kt)("h3",{id:"3b-values-enable-slurm-db"},"3.b. Values: Enable SLURM DB"),(0,a.kt)("p",null,"Edit the ",(0,a.kt)("inlineCode",{parentName:"p"},"elm/slurm-cluster/values-<cluster name>.yaml")," values"),(0,a.kt)("p",null,"Let's add the values to deploy a SLURM DB."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},"db:\n  enabled: true\n\n  config:\n    secretName: slurmdbd-conf-secret\n")),(0,a.kt)("p",null,"If you are using LDAPS and the CA is private:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},"db:\n  enabled: true\n\n  config:\n    secretName: slurmdbd-conf-secret\n\n  command: ['sh', '-c', 'update-ca-trust && /init']\n\n  volumeMounts:\n    - name: ca-cert\n      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem\n      subPath: example.com.ca.pem\n\n  volumes:\n    - name: ca-cert\n      secret:\n        secretName: local-ca-secret\n")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"local-ca-secret")," is a Secret containing ",(0,a.kt)("inlineCode",{parentName:"p"},"example.com.ca.pem"),"."),(0,a.kt)("p",null,"You can already deploy it:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},'git add .\ngit commit -m "Added SLURM DB values"\ngit push\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"# This is optional if the application is already deployed.\nkubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml\n")),(0,a.kt)("p",null,"The service should be accessible at the address ",(0,a.kt)("inlineCode",{parentName:"p"},"slurm-cluster-<cluster name>-db-0.slurm-cluster.svc.cluster.local"),". Use that URL in the slurm config."),(0,a.kt)("h2",{id:"4-slurm-controller-deployment"},"4. Slurm Controller Deployment"),(0,a.kt)("h3",{id:"4a-volumes"},"4.a. Volumes"),(0,a.kt)("p",null,"We will use NFS. Feel free to use another type of storage."),(0,a.kt)(r.Z,{groupId:"volume",mdxType:"Tabs"},(0,a.kt)(s.Z,{value:"storage-class",label:"StorageClass (dynamic)",default:!0,mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/volumes/controller-state-<cluster name>-nfs.yaml"',title:'"argo/slurm-cluster/volumes/controller-state-<cluster','name>-nfs.yaml"':!0},"apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: controller-state-<cluster name>-nfs\n  namespace: slurm-cluster\n  labels:\n    app: slurm-controller\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\nprovisioner: nfs.csi.k8s.io\nparameters:\n  server: <FILL ME> # IP or host\n  share: <FILL ME> # /srv/nfs/k8s/slurmctl\n  mountPermissions: '0775'\nmountOptions:\n  - hard\n  - nfsvers=4.1\n  - noatime\n  - nodiratime\nvolumeBindingMode: Immediate\nreclaimPolicy: Retain\nallowedTopologies:\n  - matchLabelExpressions:\n      - key: topology.kubernetes.io/region\n        values:\n          - <FILL ME> # <country code>-<city>\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/volumes/controller-state-<cluster name>-nfs.yaml\n"))),(0,a.kt)(s.Z,{value:"persistent-volume",label:"PersistentVolume (static)",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/volumes/controller-state-<cluster name>-pv.yaml"',title:'"argo/slurm-cluster/volumes/controller-state-<cluster','name>-pv.yaml"':!0},"apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: controller-state-<cluster name>-pv\n  namespace: slurm-cluster\n  labels:\n    app: slurm-controller\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\nspec:\n  capacity:\n    storage: 10Gi\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n    - noatime\n    - nodiratime\n  csi:\n    driver: nfs.csi.k8s.io\n    readOnly: false\n    volumeHandle: <unique id> # uuidgen\n    volumeAttributes:\n      server: <FILL ME> # IP or host\n      share: <FILL ME> # /srv/nfs/k8s/slurmctl\n      mountPermissions: '0775'\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/volumes/controller-state-<cluster name>-pv.yaml\n")),(0,a.kt)("p",null,"The label ",(0,a.kt)("inlineCode",{parentName:"p"},"app=slurm-controller")," will be used by the PersistentVolumeClaim."))),(0,a.kt)("h3",{id:"4b-values-enable-slurm-controller"},"4.b. Values: Enable SLURM Controller"),(0,a.kt)("p",null,"Let's add the values to deploy a SLURM Controller."),(0,a.kt)(r.Z,{groupId:"volume",mdxType:"Tabs"},(0,a.kt)(s.Z,{value:"storage-class",label:"StorageClass (dynamic)",default:!0,mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},"controller:\n  enabled: true\n\n  persistence:\n    storageClassName: 'controller-state-<cluster name>-nfs'\n    accessModes: ['ReadWriteOnce']\n    size: 10Gi\n\n  nodeSelector:\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\n\n  resources:\n    requests:\n      cpu: '250m'\n      memory: '1Gi'\n    limits:\n      cpu:\n      memory: '1Gi'\n"))),(0,a.kt)(s.Z,{value:"persistent-volume",label:"PersistentVolume (static)",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},"controller:\n  enabled: true\n\n  persistence:\n    storageClassName: ''\n    accessModes: ['ReadWriteOnce']\n    size: 10Gi\n    selectorLabels:\n      app: slurm-controller\n      topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n      topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\n\n  nodeSelector:\n    kubernetes.io/hostname: <FILL ME>\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\n\n  resources:\n    requests:\n      cpu: '250m'\n      memory: '1Gi'\n    limits:\n      cpu:\n      memory: '1Gi'\n")))),(0,a.kt)("p",null,"Notice that ",(0,a.kt)("inlineCode",{parentName:"p"},"kubernetes.io/hostname")," is used, this is because the slurm controller will be using the host network and we don't want to make the slurm controller move around."),(0,a.kt)("p",null,"We might develop a HA setup in the future version of ClusterFactory."),(0,a.kt)("p",null,"If you are using LDAPS and the CA is private, append these values:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},"controller:\n  # ...\n  command: ['sh', '-c', 'update-ca-trust && /init']\n\n  volumeMounts:\n    - name: ca-cert\n      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem\n      subPath: example.com.ca.pem\n\n  volumes:\n    - name: ca-cert\n      secret:\n        secretName: local-ca-secret\n")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"local-ca-secret")," is a Secret containing ",(0,a.kt)("inlineCode",{parentName:"p"},"example.com.ca.pem"),"."),(0,a.kt)("p",null,"You can already deploy it:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},'git add .\ngit commit -m "Added SLURM Controller values"\ngit push\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"# This is optional if the application is already deployed.\nkubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml\n")),(0,a.kt)("admonition",{type:"note"},(0,a.kt)("p",{parentName:"admonition"},"The SLURM controller is in host mode using ",(0,a.kt)("inlineCode",{parentName:"p"},"hostPort")," so it can communicate with the bare-metal hosts. There\nis also a SLURM controller ",(0,a.kt)("inlineCode",{parentName:"p"},"Service")," running for the internal communication with Slurm DB and Slurm Login.")),(0,a.kt)("h3",{id:"4c-testing-sinfo-from-the-controller-node"},"4.c Testing: ",(0,a.kt)("inlineCode",{parentName:"h3"},"sinfo")," from the controller node"),(0,a.kt)("p",null,"You should be able to run a ",(0,a.kt)("inlineCode",{parentName:"p"},"kubectl exec")," session on the controller node and execute ",(0,a.kt)("inlineCode",{parentName:"p"},"sinfo"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local"',title:'"user@local"'},"[user@local /]> kubectl exec -it -n slurm-cluster slurm-cluster-<cluster-name>-controller-0 -c slurm-cluster-<cluster-name>-controller -- bash\n\n[root@slurm-cluster-reindeer-controller-0 /]> sinfo\nPARTITION   AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmain*          up   infinite     12  down* cn[1-12]\n")),(0,a.kt)("h2",{id:"5-slurm-compute-bare-metal-deployment"},"5. Slurm Compute Bare-Metal Deployment"),(0,a.kt)("h3",{id:"5a-build-an-os-image-with-slurm"},"5.a. Build an OS Image with Slurm"),(0,a.kt)("p",null,"We have enabled ",(0,a.kt)("inlineCode",{parentName:"p"},"config-less")," in the ",(0,a.kt)("inlineCode",{parentName:"p"},"slurm.conf"),"."),(0,a.kt)("p",null,"We need to build an OS Image with Slurm Daemon installed."),(0,a.kt)("p",null,"Using the ",(0,a.kt)("inlineCode",{parentName:"p"},"packer-recipes")," directory, we can create a recipe called ",(0,a.kt)("inlineCode",{parentName:"p"},"compute.my-cluster.json"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="packer-recipes/rocky/compute.my-cluster.json"',title:'"packer-recipes/rocky/compute.my-cluster.json"'},'{\n  "variables": {\n    "boot_wait": "3s",\n    "disk_size": "50G",\n    "iso_checksum": "53a62a72881b931bdad6b13bcece7c3a2d4ca9c4a2f1e1a8029d081dd25ea61f",\n    "iso_url": "https://download.rockylinux.org/vault/rocky/8.4/isos/x86_64/Rocky-8.4-x86_64-boot.iso",\n    "memsize": "8192",\n    "numvcpus": "8"\n  },\n  "builders": [\n    {\n      "type": "qemu",\n      "accelerator": "kvm",\n      "communicator": "none",\n      "boot_command": [\n        "<up><tab><bs><bs><bs><bs><bs> ",\n        "inst.ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ks.my-cluster.cfg ",\n        "inst.cmdline",\n        "<enter><wait>"\n      ],\n      "boot_wait": "{{ user `boot_wait` }}",\n      "disk_size": "{{ user `disk_size` }}",\n      "iso_url": "{{ user `iso_url` }}",\n      "iso_checksum": "{{ user `iso_checksum` }}",\n      "headless": true,\n      "cpus": "{{ user `numvcpus` }}",\n      "memory": "{{ user `memsize` }}",\n      "vnc_bind_address": "0.0.0.0",\n      "shutdown_timeout": "3h",\n      "shutdown_timeout": "1h",\n      "qemuargs": [["-serial", "stdio"]]\n    }\n  ]\n}\n')),(0,a.kt)("p",null,"Create also the ",(0,a.kt)("inlineCode",{parentName:"p"},"ks.my-cluster.cfg")," in the ",(0,a.kt)("inlineCode",{parentName:"p"},"http")," directory:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="packer-recipes/rocky/http/ks.my-cluster.cfg"',title:'"packer-recipes/rocky/http/ks.my-cluster.cfg"'},'url --url="https://dl.rockylinux.org/pub/rocky/9.0/BaseOS/x86_64/os/"\n# License agreement\neula --agreed\n# Disable Initial Setup on first boot\nfirstboot --disable\n# Poweroff after the install is finished\npoweroff\n# Firewall\nfirewall --disable\n# Disable Initial Setup on first boot\nfirstboot --disable\nignoredisk --only-use=vda\n# Use SSSD\nauthselect select sssd with-mkhomedir with-sudo\n# System language\nlang en_US.UTF-8\n# Keyboard layout\nkeyboard us\n# Network information\nnetwork --bootproto=dhcp --device=eth0\n# SELinux configuration\nselinux --disabled\n# System timezone\ntimezone UTC --utc\n# System bootloader configuration\nbootloader --location=mbr --driveorder="vda" --timeout=1\n# Root password\nrootpw --plaintext an_example_of_default_password\n# System services\nservices --enabled="chronyd"\n\nrepo --name="AppStream" --baseurl=https://dl.rockylinux.org/pub/rocky/9.0/AppStream/x86_64/os/\nrepo --name="Extras" --baseurl=https://dl.rockylinux.org/pub/rocky/9.0/extras/x86_64/os/\nrepo --name="CRB" --baseurl=https://dl.rockylinux.org/pub/rocky/9.0/CRB/x86_64/os/\nrepo --name="epel" --baseurl=https://mirror.init7.net/fedora/epel/9/Everything/x86_64/\nrepo --name="deepsquare" --baseurl=https://yum.deepsquare.run/9/\n\n# Clear the Master Boot Record\nzerombr\n# Remove partitions\nclearpart --all --initlabel\n# Automatically create partition\npart / --size=1 --grow --asprimary --fstype=xfs\n\n# Postinstall\n%post --erroronfail\nset -ex\nmkdir /opt/xcat\n\n# Install xCat provisioning service\ncurl -fsSL "https://raw.githubusercontent.com/xcat2/xcat-core/master/xCAT/postscripts/xcatpostinit1.netboot" -o /opt/xcat/xcatpostinit1\nchmod 755 /opt/xcat/xcatpostinit1\n\ncurl -fsSL "https://raw.githubusercontent.com/xcat2/xcat-core/master/xCAT/postscripts/xcatpostinit1.service" -o /etc/systemd/system/xcatpostinit1.service\nln -s "../xcatpostinit1.service" "/etc/systemd/system/multi-user.target.wants/xcatpostinit1.service"\n\n# Postinstall\n\n#-- Pam mkhomedir: auto create home folder for ldap users\nsed -Ei \'s|UMASK\\t+[0-9]+|UMASK\\t\\t027|g\' /etc/login.defs\n\n#-- Secure umask for newly users\necho \'umask 0027\' >> /etc/profile\n\n# Kickstart copies install boot options. Serial is turned on for logging with\n# Packer which disables console output. Disable it so console output is shown\n# during deployments\nsed -i \'s/^GRUB_TERMINAL=.*/GRUB_TERMINAL_OUTPUT="console"/g\' /etc/default/grub\nsed -i \'/GRUB_SERIAL_COMMAND="serial"/d\' /etc/default/grub\nsed -ri \'s/(GRUB_CMDLINE_LINUX=".*)\\s+console=ttyS0(.*")/\\1\\2/\' /etc/default/grub\n\n# Clean up install config not applicable to deployed environments.\nfor f in resolv.conf fstab; do\n    rm -f /etc/$f\n    touch /etc/$f\n    chown root:root /etc/$f\n    chmod 644 /etc/$f\ndone\n\ncat << EOF >>/etc/fstab\ndevpts  /dev/pts devpts   gid=5,mode=620 0 0\ntmpfs   /dev/shm tmpfs    defaults       0 0\nproc    /proc    proc     defaults       0 0\nsysfs   /sys     sysfs    defaults       0 0\nEOF\n\nrm -f /etc/sysconfig/network-scripts/ifcfg-[^lo]*\n\ndnf clean all\n%end\n\n%packages\n@minimal-environment\nchrony\n\n# kernel\nkernel-5.14.0-70.22.1.el9_0.x86_64\nkernel-devel-5.14.0-70.22.1.el9_0.x86_64\nkernel-headers-5.14.0-70.22.1.el9_0.x86_64\nkernel-tools-5.14.0-70.22.1.el9_0.x86_64\nkernel-modules-5.14.0-70.22.1.el9_0.x86_64\nkernel-core-5.14.0-70.22.1.el9_0.x86_64\nkernel-modules-extra-5.14.0-70.22.1.el9_0.x86_64\n\nbash-completion\ncloud-init\n# cloud-init only requires python3-oauthlib with MAAS. As such upstream\n# removed this dependency.\npython3-oauthlib\nrsync\ntar\n\n# disk growing\ncloud-utils-growpart\n\n# grub2-efi-x64 ships grub signed for UEFI secure boot. If grub2-efi-x64-modules\n# is installed grub will be generated on deployment and unsigned which breaks\n# UEFI secure boot.\ngrub2-efi-x64\nefibootmgr\nshim-x64\ndosfstools\nlvm2\nmdadm\ndevice-mapper-multipath\niscsi-initiator-utils\n\ndnf-plugins-core\n\n# other packages\nnet-tools\nnfs-utils\nopenssh-server\nrsync\ntar\nutil-linux\nwget\npython3\ntar\nbzip2\nbc\ndracut\ndracut-network\nrsyslog\nhostname\ne2fsprogs\nethtool\nparted\nopenssl\ndhclient\nopenssh-clients\nbash\nvim-minimal\nrpm\niputils\nperl-interpreter\ngawk\nxz\nsquashfs-tools\ncpio\nsudo\nmake\nbash-completion\nnano\npciutils\ngit\nmlocate\nsssd\nvim-enhanced\nsystemd-udev\nnumactl\nmunge\nlibevent-devel\ntmux\noddjob\noddjob-mkhomedir\nredis\nunzip\nnmap\nflex\ntk\nbison\nlibgfortran\ntcl\ngcc-gfortran\nlibcurl\nlibnl3-devel\npython39\nnumactl-libs\nxfsprogs\nzsh\n#pkgconf-pkg-config\nrpm-build\nhwloc\nhwloc-libs\nhwloc-devel\ntcsh\nksh\nxorg-x11-fonts-ISO8859-1-75dpi.noarch\nxorg-x11-fonts-cyrillic.noarch\n\n# otherpkgs\nhtop\npmix4\nslurm\nslurm-contribs\nslurm-libpmi\nslurm-pam_slurm\nslurm-slurmd\n# beeond build dependency\nelfutils-libelf-devel\n\n-plymouth\n# Remove Intel wireless firmware\n-i*-firmware\n%end\n\n')),(0,a.kt)("p",null,"Build the image with:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory/packer-recipes/rocky"',title:'"user@local:/ClusterFactory/packer-recipes/rocky"'},"packer build compute.my-cluster.json\n")),(0,a.kt)("p",null,"And send the os image to xcat. ",(0,a.kt)("a",{parentName:"p",href:"/docs/guides/provisioning/packer-build"},'Follow the guide "Build an OS Image with Packer" for more details'),"."),(0,a.kt)("h3",{id:"5b-xcat-postbootscripts"},"5.b. xCAT Postbootscripts"),(0,a.kt)("p",null,"Next, you have to configure a service by using a xCAT postscript. Our recommendation is to use a xCAT postscript to pull a Git repository which, based on the content of the repository, copies the files and executes the postscripts in that Git repository."),(0,a.kt)("p",null,"This way, the GitOps practice is always followed and permits to adapt for the future version of ClusterFactory."),(0,a.kt)("p",null,"The service:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-properties",metastring:'title="/etc/systemd/system/slurmd.service"',title:'"/etc/systemd/system/slurmd.service"'},"[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service\n\n[Service]\nType=forking\nExecStartPre=/usr/bin/id slurm\nRestart=always\nRestartSec=3\nExecStart=/usr/sbin/slurmd -d /usr/sbin/slurmstepd --conf-server <node host IP>\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\n\n[Install]\nWantedBy=multi-user.target\n")),(0,a.kt)("p",null,"A simple postbootscript:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="sample-configure-slurm.sh',title:'"sample-configure-slurm.sh'},"#!/bin/sh -ex\nmkdir -p /var/log/slurm/\n\ncat <<\\END | base64 -d >/etc/munge/munge.key\n...\nEND\n\nchmod 600 /etc/munge/munge.key\n\ncat <<\\END >/etc/sssd/sssd.conf\n...\nEND\n\nchmod 600 /etc/sssd/sssd.conf\n\n#-- Add enroot extra hooks for PMIx and PyTorch multi-node support\ncp /usr/share/enroot/hooks.d/50-slurm-pmi.sh /usr/share/enroot/hooks.d/50-slurm-pytorch.sh /etc/enroot/hooks.d\n\n# Enable Pyxis (container jobs)\ncat <<\\END >/etc/slurm/plugstack.conf.d/\noptional /usr/lib64/slurm/spank_pyxis.so runtime_path=/run/pyxis container_scope=job\nEND\n\ncat <<\\END >/etc/systemd/system/slurmd.service\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nWants=network-online.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=3\nExecStart=/usr/sbin/slurmd -d /usr/sbin/slurmstepd --conf-server <node host IP>\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd.pid\nKillMode=process\nLimitNOFILE=131072\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\nStandardOutput=null\nStandardError=null\n\n[Install]\nWantedBy=multi-user.target\nEND\n\n#-- Wait for LDAP\nupdate-ca-trust\nsystemctl restart sssd\nwhile ! id slurm\ndo\n  sleep 1\ndone\n\nsystemctl daemon-reload\nsystemctl restart munge\nsystemctl enable slurmd\nsystemctl start --no-block slurmd\n")),(0,a.kt)("p",null,"After setup SLURM, you should also:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Mount the home directory of the LDAP users (probably like ",(0,a.kt)("inlineCode",{parentName:"li"},"/home/ldap-users"),")"),(0,a.kt)("li",{parentName:"ul"},"Use the postscript to configure SSSD"),(0,a.kt)("li",{parentName:"ul"},"Use the postscript to import the ",(0,a.kt)("inlineCode",{parentName:"li"},"munge.key"))),(0,a.kt)("h3",{id:"5c-reboot-the-nodes"},"5.c. Reboot the nodes"),(0,a.kt)("p",null,"If the controller is running, the nodes should automatically receive the ",(0,a.kt)("inlineCode",{parentName:"p"},"slurm.conf")," inside ",(0,a.kt)("inlineCode",{parentName:"p"},"/run/slurm/conf"),"."),(0,a.kt)("h2",{id:"6-slurm-login-deployment"},"6. Slurm Login Deployment"),(0,a.kt)("h3",{id:"6a-secrets-and-volumes"},"6.a. Secrets and Volumes"),(0,a.kt)("h4",{id:"ssh-server-configuration"},"SSH Server configuration"),(0,a.kt)("p",null,"The login nodes are exposed to the external network using Multus CNI and the IPVLAN plugin. This is to expose the ",(0,a.kt)("inlineCode",{parentName:"p"},"srunPortRange")," and the SSH port.\nThanks to SSSD, the users can log in to the nodes using SSH using the passwords stored in LDAP."),(0,a.kt)("p",null,"We have to generate the SSH host keys:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local"',title:'"user@local"'},"yes 'y' | ssh-keygen -N '' -f ./ssh_host_rsa_key -t rsa -C login-node\nyes 'y' | ssh-keygen -N '' -f ./ssh_host_ecdsa_key -t ecdsa -C login-node\nyes 'y' | ssh-keygen -N '' -f ./ssh_host_ed25519_key -t ed25519 -C login-node\n")),(0,a.kt)("p",null,"6 files will be generated. We will also add our ",(0,a.kt)("inlineCode",{parentName:"p"},"sshd_config"),"."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Create a ",(0,a.kt)("inlineCode",{parentName:"li"},"-secret.yaml.local")," file:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/login-sshd-secret.yaml.local"',title:'"argo/slurm-cluster/secrets/login-sshd-secret.yaml.local"'},"apiVersion: v1\nkind: Secret\nmetadata:\n  name: login-sshd-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  ssh_host_ecdsa_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    <FILL ME>\n    -----END OPENSSH PRIVATE KEY-----\n  ssh_host_ecdsa_key.pub: |\n    ecdsa-sha2-nistp256 <FILL ME>\n  ssh_host_ed25519_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    <FILL ME>\n    -----END OPENSSH PRIVATE KEY-----\n  ssh_host_ed25519_key.pub: |\n    ssh-ed25519 <FILL ME>\n  ssh_host_rsa_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    <FILL ME>\n    -----END OPENSSH PRIVATE KEY-----\n  ssh_host_rsa_key.pub: |\n    ssh-rsa <FILL ME>\n  sshd_config: |\n    Port 22\n    AddressFamily any\n    ListenAddress 0.0.0.0\n    ListenAddress ::\n\n    HostKey /etc/ssh/ssh_host_rsa_key\n    HostKey /etc/ssh/ssh_host_ecdsa_key\n    HostKey /etc/ssh/ssh_host_ed25519_key\n\n    PermitRootLogin prohibit-password\n    PasswordAuthentication yes\n\n    # Change to yes to enable challenge-response passwords (beware issues with\n    # some PAM modules and threads)\n    ChallengeResponseAuthentication no\n\n    UsePAM yes\n\n    X11Forwarding yes\n    PrintMotd no\n    AcceptEnv LANG LC_*\n\n    # override default of no subsystems\n    Subsystem sftp  /usr/lib/openssh/sftp-server\n\n    AuthorizedKeysCommand /usr/bin/sss_ssh_authorizedkeys\n    AuthorizedKeysCommandUser root\n")),(0,a.kt)("p",null,"Replace the ",(0,a.kt)("inlineCode",{parentName:"p"},"<FILL ME>")," with the values based on the generated files."),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},"Seal the secret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"cfctl kubeseal\n")),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},"Apply the SealedSecret:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/secrets/login-sshd-sealed-secret.yaml\n")),(0,a.kt)("h4",{id:"home-directory-for-the-ldap-users"},"Home directory for the LDAP users"),(0,a.kt)("p",null,"If you have configured your LDAP server, you might have to change the ",(0,a.kt)("inlineCode",{parentName:"p"},"homeDirectory")," to something like ",(0,a.kt)("inlineCode",{parentName:"p"},"/home/ldap-users"),"."),(0,a.kt)("p",null,"You must mount the home directory of the LDAP users using NFS."),(0,a.kt)("p",null,"DO NOT use ",(0,a.kt)("inlineCode",{parentName:"p"},"StorageClass")," since the provisioning is static. We don't want to create a volume per replica. There is only one common volume."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="argo/slurm-cluster/volumes/ldap-users-<cluster name>-pv.yaml"',title:'"argo/slurm-cluster/volumes/ldap-users-<cluster','name>-pv.yaml"':!0},"apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ldap-users-<cluster name>-pv\n  namespace: slurm-cluster\n  labels:\n    app: slurm-login\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\nspec:\n  capacity:\n    storage: 1000Gi\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n    - noatime\n    - nodiratime\n  csi:\n    driver: nfs.csi.k8s.io\n    readOnly: false\n    volumeHandle: <unique id> # uuidgen\n    volumeAttributes:\n      server: <FILL ME> # IP or host\n      share: <FILL ME> # /srv/nfs/k8s/ldap-users\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ldap-users-<cluster name>-pvc\n  namespace: slurm-cluster\n  labels:\n    app: slurm-login\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\nspec:\n  volumeName: ldap-users-<cluster name>-pv\n  accessModes: [ReadWriteMany]\n  storageClassName: ''\n  resources:\n    requests:\n      storage: 1000Gi\n")),(0,a.kt)("p",null,"Apply the PV and PVC:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"kubectl apply -f argo/slurm-cluster/volumes/ldap-users-<cluster name>-pv.yaml\n")),(0,a.kt)("h3",{id:"6b-values-enable-slurm-login"},"6.b. Values: Enable SLURM Login"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},"login:\n  enabled: true\n  replicas: 1\n\n  sshd:\n    secretName: login-sshd-secret\n\n  nodeSelector:\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\n\n  # Extra volume mounts\n  volumeMounts:\n    - name: ldap-users-pvc\n      mountPath: /home/ldap-users\n\n  # Extra volumes\n  volumes:\n    - name: ldap-users-pvc\n      persistentVolumeClaim:\n        claimName: ldap-users-<cluster name>-pvc\n\n  net:\n    # Kubernetes host interface\n    masterInterface: eth0\n    mode: l2\n    type: ipvlan\n\n    # https://www.cni.dev/plugins/current/ipam/static/\n    ipam:\n      type: host-local\n      ranges:\n        - - subnet: 192.168.0.0/24\n            rangeStart: 192.168.0.20\n            rangeEnd: 192.168.0.21\n            gateway: 192.168.0.1\n")),(0,a.kt)("p",null,"Edit the values accordingly."),(0,a.kt)("admonition",{type:"warning"},(0,a.kt)("p",{parentName:"admonition"},"Because ",(0,a.kt)("inlineCode",{parentName:"p"},"k8s-pod-network")," is the default network, you must write routes to your networks."),(0,a.kt)("p",{parentName:"admonition"},"For example, if we have two sites ",(0,a.kt)("inlineCode",{parentName:"p"},"10.10.0.0/24")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"10.10.1.0/24"),", you would write:"),(0,a.kt)("pre",{parentName:"admonition"},(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"ipam:\n  type: host-local\n  ranges:\n    - - subnet: 10.10.0.0/24\n        rangeStart: 10.10.0.20\n        rangeEnd: 10.10.0.21\n        gateway: 10.10.0.1\n  routes:\n    - dst: 10.10.1.0/24\n")),(0,a.kt)("p",{parentName:"admonition"},"If we ",(0,a.kt)("inlineCode",{parentName:"p"},"kubectl exec")," to the container and run ",(0,a.kt)("inlineCode",{parentName:"p"},"ip route"),", you would see:"),(0,a.kt)("pre",{parentName:"admonition"},(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="root@slurm-cluster-e<cluster name>-login-b9b7cd9d5-9ntkn"',title:'"root@slurm-cluster-e<cluster','name>-login-b9b7cd9d5-9ntkn"':!0},"# ip route\ndefault via 169.254.1.1 dev eth0\n10.10.0.0/24 via 10.10.0.1 dev net1\n10.10.1.0/24 via 10.10.0.1 dev net1\n169.254.1.1 dev eth0 scope link\n10.10.0.0/20 via 10.10.0.1 dev net1\n10.10.0.0/20 dev net1 proto kernel scope link src 10.10.0.21\n")),(0,a.kt)("p",{parentName:"admonition"},"The issue is tracked at ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/SquareFactory/ClusterFactory/issues/29"},"SquareFactory/ClusterFactory#29")," and ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/projectcalico/calico/issues/5199"},"projectcalico/calico#5199"),".")),(0,a.kt)("p",null,"If you are using LDAPS and the CA is private, add these values:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',title:'"helm/slurm-cluster/values-<cluster','name>.yaml"':!0},"login:\n  # ...\n  command: ['sh', '-c', 'update-ca-trust && /init']\n\n  volumeMounts:\n    - name: ldap-users-pvc\n      mountPath: /home/ldap-users\n    - name: ca-cert\n      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem\n      subPath: example.com.ca.pem\n\n  volumes:\n    - name: ldap-users-pvc\n      persistentVolumeClaim:\n        claimName: ldap-users-<cluster name>-pvc\n    - name: ca-cert\n      secret:\n        secretName: local-ca-secret\n")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"local-ca-secret")," is a Secret containing ",(0,a.kt)("inlineCode",{parentName:"p"},"example.com.ca.pem"),"."),(0,a.kt)("p",null,"You can deploy it:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},'git add .\ngit commit -m "Added SLURM Login values"\ngit push\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local:/ClusterFactory"',title:'"user@local:/ClusterFactory"'},"# This is optional if the application is already deployed.\nkubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml\n")),(0,a.kt)("h3",{id:"6c-testing-access-to-a-slurm-login-node"},"6.c Testing: Access to a SLURM Login node"),(0,a.kt)("p",null,"Because the container is exposed to the external network, you should be able to ssh directly to the login node."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:'title="user@local"',title:'"user@local"'},"ssh user@login-node\n")),(0,a.kt)("p",null,"If the LDAP User ",(0,a.kt)("inlineCode",{parentName:"p"},"user")," exists, the login node should be asking for a password."))}d.isMDXComponent=!0},3210:(e,t,n)=>{n.d(t,{Z:()=>l});const l=n.p+"assets/images/image-20220512151655613-a88d0547e2bc1fb1d959aa9d46383a2e.png"}}]);