# Deploying SLURM Cluster

![image-20220512151655613](01-deploy-slurm.assets/image-20220512151655613.png)

:::warning

Deploying the SLURM database isn't stable yet. Please feel free to [create an issue](https://github.com/SquareFactory/cluster-factory-ce/issues/new) so we can improve its stability.

:::

**Compared to the other guides we will start from scratch.**

Delete the `argo/slurm-cluster` directory (or rename it).

## Preparation

Deploying a SLURM cluster isn't easy and you MUST have these components ready:

- A LDAP server and a SSSD configuration, to synchronize the user ID across the cluster
- A MySQL server for the SLURM DB
- A JWT private key, for the authentication via REST API
- A MUNGE key, for the authentication of SLURM daemons

### LDAP deployment

Follow [the guide](/docs/guides/deploy-ldap).

### SSSD configuration

Let's store it in a `Secret`:

1. Create the `argo/slurm-cluster/secrets/` directory and create a `-secret.yml.local` file:

```yaml title="argo/slurm-cluster/secrets/sssd-secret.yaml.local"
apiVersion: v1
kind: Secret
metadata:
  name: sssd-secret
  namespace: slurm-cluster
type: Opaque
stringData:
  jwt_hs256.key: |
  sssd.conf: |
    # https://sssd.io/docs/users/troubleshooting/how-to-troubleshoot-backend.html
    [sssd]
    services = nss,pam,sudo,ssh
    config_file_version = 2
    domains = ldap

    [sudo]

    [nss]

    [pam]
    offline_credentials_expiration = 60

    [domain/ldap]
    #debug_level = 0x3ff0   <= perfect for debugging

    id_provider = ldap
    access_provider = ldap
    cache_credentials = True

    ldap_uri = <LDAP/LDAPS uri>
    ldap_default_bind_dn = <FILL ME: bind DN>
    ldap_default_authtok = <FILL ME: bind password>
    ldap_search_timeout = 50
    ldap_network_timeout = 60

    ldap_search_base = ou=People,dc=example,dc=com
    ldap_group_search_base = ou=Group,dc=example,dc=com
    ldap_sudo_search_base = ou=SUDOers,dc=example,dc=com
    ldap_user_extra_attrs = publicSSHKey
    ldap_user_ssh_public_key = publicSSHKey

    ldap_access_order = filter
    ldap_access_filter = (objectClass=posixAccount)

    ldap_tls_cipher_suite = HIGH
    # On Ubuntu, the LDAP client is linked to GnuTLS instead of OpenSSL => cipher suite names are different
    # In fact, it's not even a cipher suite name that goes here, but a so called "priority list" => see $> gnutls-cli --priority-list
    # See https://backreference.org/2009/11/18/openssl-vs-gnutls-cipher-names/ , gnutls-cli is part of package gnutls-bin
```

2. Seal the secret:

```shell title="user@local:/cluster-factory-ce"
./kubeseal-every-local-files.sh
```

3. Apply the SealedSecret:

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/secrets/sssd-sealed-secret.yaml
```

### MySQL deployment

You can deploy MySQL using the [Helm Chart of Bitnami](https://bitnami.com/stack/mysql/helm) and develop an [Argo CD app](/docs/guides/develop-apps-for-cluster-factory).

### JWT Key generation

```shell title="user@local"
ssh-keygen -t rsa -b 4096 -m PEM -f jwtRS256.key
```

Let's store it in a `Secret`:

1.Create a `-secret.yml.local` file:

```yaml title="argo/slurm-cluster/secrets/slurm-secret.yaml.local"
apiVersion: v1
kind: Secret
metadata:
  name: slurm-secret
  namespace: slurm-cluster
type: Opaque
stringData:
  jwt_hs256.key: |
    -----BEGIN RSA PRIVATE KEY-----
    ...
    -----END RSA PRIVATE KEY-----
```

2. Seal the secret:

```shell title="user@local:/cluster-factory-ce"
./kubeseal-every-local-files.sh
```

3. Apply the SealedSecret:

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/secrets/slurm-sealed-secret.yaml
```

### MUNGE Key generation

```shell title="root@local"
# As root
dnf install -y munge
/usr/sbin/create-munge-key
cat /etc/munge/munge.key | base64
```

Let's store it in a `Secret`:

1. Create a `-secret.yml.local` file:

```yaml title="argo/slurm-cluster/secrets/munge-secret.yml.local"
apiVersion: v1
kind: Secret
metadata:
  name: munge-secret
  namespace: slurm-cluster
type: Opaque
data:
  munge.key: |
    <base 64 encoded key>
```

2. Seal the secret:

```shell title="user@local:/cluster-factory-ce"
./kubeseal-every-local-files.sh
```

3. Apply the SealedSecret:

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/cvmfs/secrets/munge-sealed-secret.yml
```

## 1. Namespace and AppProject

Create and apply the `Namespace` and `AppProject`:

```yaml title="argo/slurm-cluster/namespace.yml"
apiVersion: v1
kind: Namespace
metadata:
  name: slurm-cluster
  labels:
    app.kubernetes.io/name: slurm-cluster
```

```yaml title="argo/slurm-cluster/app-project.yml"
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: slurm-cluster
  namespace: argocd
  # Finalizer that ensures that project is not deleted until it is not referenced by any application
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  description: Slurm cluster
  # Allow manifests to deploy from any Git repos
  sourceRepos:
    - '*'
  # Only permit applications to deploy to the namespace in the same cluster
  destinations:
    - namespace: slurm-cluster
      server: https://kubernetes.default.svc

  namespaceResourceWhitelist:
    - kind: '*'
      group: '*'

  clusterResourceWhitelist:
    - kind: '*'
      group: '*'
```

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/
```

## 2. Begin writing the `slurm-cluster-<cluster name>-app.yml`

### 2.a. Argo CD Application configuration

```yaml title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml"
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: slurm-cluster-<FILL ME: cluster name>-app
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: slurm-cluster
  source:
    repoURL: git@github.com:squarefactory/cluster-factory-ce.git
    targetRevision: HEAD
    path: helm/slurm-cluster
    helm:
      releaseName: slurm-cluster-<FILL ME: cluster name>

      values: '' # FILL ME

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: slurm-cluster

  syncPolicy:
    automated:
      prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).
      selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).
      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).
    syncOptions: []
    retry:
      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0
      backoff:
        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
        factor: 2 # a factor to multiply the base duration after each failed retry
        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy
```

### 2.b. Values: Configuring the SLURM cluster

Add:

```yaml title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml > spec > source > helm > values"
values: |
  sssd:
    secretName: sssd-secret

  munge:
    secretName: munge-secret

  jwt:
    secretName: slurm-secret

  slurmConfig:
    clusterName: <FILL ME: cluster-name>

    compute:
      srunPortRangeStart: 60001
      srunPortRangeEnd: 63000
      debug: debug5

    accounting: |
      AccountingStorageType=accounting_storage/slurmdbd
      AccountingStorageHost=slurm-cluster-<FILL ME: cluster name>.slurm-cluster.svc.cluster.local
      AccountingStoragePort=6819
      AccountingStorageTRES=gres/gpu

    controller:
      parameters: enable_configless
      debug: debug5

    defaultResourcesAllocation: |
      # Change accordingly
      DefCpuPerGPU=4
      DefMemPerCpu=7000

    nodes: |
      # Change accordingly
      NodeName=cn[1-12]  CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128473 Gres=gpu:4

    partitions: |
      # Change accordingly
      PartitionName=main Nodes=cn[1-12] Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"

    gres: |
      # Change accordingly
      NodeName=cn[1-12] File=/dev/nvidia[0-3] AutoDetect=nvml

    # Extra slurm.conf configuration
    extra: |
      LaunchParameters=enable_nss_slurm
      DebugFlags=Script,Gang,SelectType
      TCPTimeout=5

      # MPI stacks running over Infiniband or OmniPath require the ability to allocate more
      # locked memory than the default limit. Unfortunately, user processes on login nodes
      # may have a small memory limit (check it by ulimit -a) which by default are propagated
      # into Slurm jobs and hence cause fabric errors for MPI.
      PropagateResourceLimitsExcept=MEMLOCK

      ProctrackType=proctrack/cgroup
      TaskPlugin=task/cgroup
      SwitchType=switch/none
      MpiDefault=pmix_v2
      ReturnToService=2
      GresTypes=gpu
      PreemptType=preempt/qos
      PreemptMode=REQUEUE
      PreemptExemptTime=-1
      Prolog=/etc/slurm/prolog.d/*
      Epilog=/etc/slurm/epilog.d/*

      # Federation
      FederationParameters=fed_display
```

## 3. Slurm DB Deployment

### 3.a. Secrets

Assuming you have deployed LDAP and MySQL, we will store the `slurmdbd.conf` inside a secret:

1. Create a `-secret.yml.local` file:

```yaml title="argo/slurm-cluster/secrets/slurmdbd-conf-secret.yml.local"
apiVersion: v1
kind: Secret
metadata:
  name: slurmdbd-conf-secret
  namespace: slurm-cluster
type: Opaque
stringData:
  slurmdbd.conf: |
    # See https://slurm.schedmd.com/slurmdbd.conf.html
    ### Main
    DbdHost=slurm-cluster-<FILL ME: cluster name>-db-0
    SlurmUser=slurm

    ### Logging
    DebugLevel=debug5	# optional, defaults to 'info'. Possible values: fatal, error, info, verbose, debug, debug[2-5]
    LogFile=/var/log/slurm/slurmdbd.log
    PidFile=/var/run/slurmdbd.pid
    LogTimeFormat=thread_id

    AuthAltTypes=auth/jwt
    AuthAltParameters=jwt_key=/var/spool/slurm/jwt_hs256.key

    ### Database server configuration
    StorageType=accounting_storage/mysql
    StorageHost=<FILL ME>
    StorageUser=<FILL ME>
    StoragePass=<FILL ME>
```

Replace the `<FILL ME>` according to your existing configuration.

Choose a `Helm Release Name`. We recommend `slurm-cluster-<cluster name>`.

2. Seal the secret:

```shell title="user@local:/cluster-factory-ce"
./kubeseal-every-local-files.sh
```

3. Apply the SealedSecret:

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/secrets/slurmdbd-conf-sealed-secret.yml
```

### 3.b. Values: Enable SLURM DB

Edit the `slurm-cluster-<cluster name>-app.yml` values

Let's add the values to deploy a SLURM DB.

```yaml title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml > spec > source > helm > values"
db:
  enabled: true

  config:
    secretName: slurmdbd-conf-secret
```

If you are using LDAPS and the CA is private:

```yaml title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml > spec > source > helm > values"
db:
  enabled: true

  config:
    secretName: slurmdbd-conf-secret

  command: ['sh', '-c', 'update-ca-trust && /init']

  volumeMounts:
    - name: ca-cert
      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem
      subPath: example.com.ca.pem

  volumes:
    - name: ca-cert
      secret:
        secretName: local-ca-secret
```

`local-ca-secret` is a Secret containing `example.com.ca.pem`.

You can already deploy it:

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml
```

## 4. Slurm Controller Deployment

### 4.a. Volumes

We will use NFS. Feel free to use another type of storage.

<Tabs groupId="volume">
  <TabItem value="storage-class" label="StorageClass (dynamic)" default>

```yaml title="argo/slurm-cluster/volumes/controller-state-<cluster name>-nfs.yaml"
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: controller-state-<cluster name>-nfs
  namespace: slurm-cluster
  labels:
    app: slurm-controller
    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>
    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>
provisioner: nfs.csi.k8s.io
parameters:
  server: <FILL ME> # IP or host
  share: <FILL ME> # /srv/nfs/k8s/slurmctl
  mountPermissions: '0775'
mountOptions:
  - hard
  - nfsvers=4.1
  - noatime
  - nodiratime
volumeBindingMode: Immediate
reclaimPolicy: Retain
allowedTopologies:
  - matchLabelExpressions:
      - key: topology.kubernetes.io/zone
        values:
          - <FILL ME> # <country code>-<city>-<index>
```

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/volumes/controller-state-<cluster name>-nfs.yaml.yaml
```

  </TabItem>
  <TabItem value="persistent-volume" label="PersistentVolume (static)">

```yaml title="argo/slurm-cluster/volumes/controller-state-<cluster name>-pv.yaml"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: controller-state-<cluster name>-pv
  namespace: slurm-cluster
  labels:
    app: slurm-controller
    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>
    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>
spec:
  capacity:
    storage: 10Gi
  mountOptions:
    - hard
    - nfsvers=4.1
    - noatime
    - nodiratime
  csi:
    driver: nfs.csi.k8s.io
    readOnly: false
    volumeHandle: <unique id> # uuidgen
    volumeAttributes:
      server: <FILL ME> # IP or host
      share: <FILL ME> # /srv/nfs/k8s/slurmctl
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
```

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/volumes/controller-state-<cluster name>-pv.yaml
```

The label `app=slurm-controller` will be used by the PersistentVolumeClaim.

  </TabItem>
</Tabs>

### 4.b. Values: Enable SLURM Controller

Let's add the values to deploy a SLURM Controller.

<Tabs groupId="volume">
  <TabItem value="storage-class" label="StorageClass (dynamic)" default>

```yaml title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml > spec > source > helm > values"
controller:
  enabled: true

  persistence:
    storageClassName: 'controller-state-<cluster name>-nfs'
    accessModes: ['ReadWriteOnce']
    size: 10Gi

  nodeSelector:
    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>
    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>

  resources:
    requests:
      cpu: '250m'
      memory: '1Gi'
    limits:
      cpu:
      memory: '1Gi'
```

  </TabItem>
  <TabItem value="persistent-volume" label="PersistentVolume (static)">

```yaml title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml > spec > source > helm > values"
controller:
  enabled: true

  persistence:
    storageClassName: ''
    accessModes: ['ReadWriteOnce']
    size: 10Gi
    selectorLabels:
      app: slurm-controller
      topology.kubernetes.io/region: <FILL ME> # <country code>-<city>
      topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>

  nodeSelector:
    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>
    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>

  resources:
    requests:
      cpu: '250m'
      memory: '1Gi'
    limits:
      cpu:
      memory: '1Gi'
```

  </TabItem>
</Tabs>

If you are using LDAPS and the CA is private, append these values:

```yaml title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml > spec > source > helm > values"
controller:
  # ...

  volumeMounts:
    - name: ca-cert
      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem
      subPath: example.com.ca.pem

  volumes:
    - name: ca-cert
      secret:
        secretName: local-ca-secret
```

`local-ca-secret` is a Secret containing `example.com.ca.pem`.

You can already deploy it:

```shell title="user@local:/cluster-factory-ce"
kubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yml
```

:::note

The SLURM controller is in host mode using `hostPort` so it can communicate with the bare-metal hosts. There is also a SLURM controller service running for the internal communication (with Slurm DB and SLurm Login).

:::

## 5. Slurm Compute Bare-Metal Deployment

### 5.a. Build an OS Image with Slurm

We have enabled `configless` in the `slurm.conf`.

We need to build an OS Image with Slurm Daemon installed.

Using the `packer-recipes` directory, we can create a recipe called `compute.my-cluster.json`:

```json title="packer-recipes/rocky/compute.my-cluster.json"
{
  "variables": {
    "boot_wait": "3s",
    "disk_size": "50G",
    "iso_checksum": "53a62a72881b931bdad6b13bcece7c3a2d4ca9c4a2f1e1a8029d081dd25ea61f",
    "iso_url": "https://download.rockylinux.org/vault/rocky/8.4/isos/x86_64/Rocky-8.4-x86_64-boot.iso",
    "memsize": "8192",
    "numvcpus": "8"
  },
  "builders": [
    {
      "type": "qemu",
      "accelerator": "kvm",
      "communicator": "none",
      "boot_command": [
        "<up><tab><bs><bs><bs><bs><bs> ",
        "inst.ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ks.my-cluster.cfg ",
        "inst.cmdline",
        "<enter><wait>"
      ],
      "boot_wait": "{{ user `boot_wait` }}",
      "disk_size": "{{ user `disk_size` }}",
      "iso_url": "{{ user `iso_url` }}",
      "iso_checksum": "{{ user `iso_checksum` }}",
      "headless": true,
      "cpus": "{{ user `numvcpus` }}",
      "memory": "{{ user `memsize` }}",
      "vnc_bind_address": "0.0.0.0",
      "http_directory": "http",
      "shutdown_timeout": "1h",
      "qemuargs": [["-serial", "stdio"]]
    }
  ]
}
```

Create also the `ks.my-cluster.cfg` in the `http` directory:

```shell title="packer-recipes/rocky/http/ks.my-cluster.cfg"
url --url="https://dl.rockylinux.org/vault/rocky/8.4/BaseOS/x86_64/os/"
# License agreement
eula --agreed
# Disable Initial Setup on first boot
firstboot --disable
# Poweroff after the install is finished
poweroff
# Firewall
firewall --enabled --service=ssh
# Disable Initial Setup on first boot
firstboot --disable
ignoredisk --only-use=vda
# System language
lang en_US.UTF-8
# Keyboard layout
keyboard us
# Network information
network --bootproto=dhcp --device=eth0
# SELinux configuration
selinux --disabled
# System timezone
timezone UTC --isUtc
# System bootloader configuration
bootloader --location=mbr --driveorder="vda" --timeout=1
# Root password
rootpw --plaintext an_example_of_default_password
# System services
services --enabled="chronyd"

repo --name="AppStream" --baseurl=https://dl.rockylinux.org/vault/rocky/8.4/AppStream/x86_64/os/
repo --name="Extras" --baseurl=https://dl.rockylinux.org/vault/rocky/8.4/extras/x86_64/os/
repo --name="PowerTools" --baseurl=https://dl.rockylinux.org/vault/rocky/8.4/PowerTools/x86_64/os/
repo --name="epel" --baseurl=https://mirror.init7.net/fedora/epel/8/Everything/x86_64/
repo --name="csquare" --baseurl=https://csquare:VCyV78sQ@yum.csquare.run/
repo --name="beegfs" --baseurl=https://www.beegfs.io/release/beegfs_7.2.6/dists/rhel8/

# Clear the Master Boot Record
zerombr
# Remove partitions
clearpart --all --initlabel
# Automatically create partition
part / --size=1 --grow --asprimary --fstype=xfs

# Postinstall
%post --erroronfail
set -ex
mkdir /opt/xcat

cat << EOF >>/etc/fstab
devpts  /dev/pts devpts   gid=5,mode=620 0 0
tmpfs   /dev/shm tmpfs    defaults       0 0
proc    /proc    proc     defaults       0 0
sysfs   /sys     sysfs    defaults       0 0
EOF

# Postinstall
#-- No firewall
systemctl disable firewalld

#-- Pam mkhomedir: auto create home folder for ldap users
authselect select sssd \
  with-mkhomedir \
  with-sudo \
  --force --quiet
sed -Ei 's|UMASK\t+[0-9]+|UMASK\t\t027|g' /etc/login.defs

#-- Secure umask for newly users
echo 'umask 0027' >> /etc/profile

# Kickstart copies install boot options. Serial is turned on for logging with
# Packer which disables console output. Disable it so console output is shown
# during deployments
sed -i 's/^GRUB_TERMINAL=.*/GRUB_TERMINAL_OUTPUT="console"/g' /etc/default/grub
sed -i '/GRUB_SERIAL_COMMAND="serial"/d' /etc/default/grub
sed -ri 's/(GRUB_CMDLINE_LINUX=".*)\s+console=ttyS0(.*")/\1\2/' /etc/default/grub

# Clean up install config not applicable to deployed environments.
for f in resolv.conf; do
    rm -f /etc/$f
    touch /etc/$f
    chown root:root /etc/$f
    chmod 644 /etc/$f
done

rm -f /etc/sysconfig/network-scripts/ifcfg-[^lo]*

dnf clean all
%end

%packages
@minimal-environment
chrony

# kernel
kernel-4.18.0-305.17.1.el8_4.x86_64
kernel-devel-4.18.0-305.17.1.el8_4.x86_64
kernel-headers-4.18.0-305.17.1.el8_4.x86_64
kernel-tools-4.18.0-305.17.1.el8_4.x86_64
kernel-modules-4.18.0-305.17.1.el8_4.x86_64
kernel-core-4.18.0-305.17.1.el8_4.x86_64
kernel-modules-extra-4.18.0-305.17.1.el8_4.x86_64

bash-completion
cloud-init
# cloud-init only requires python3-oauthlib with MAAS. As such upstream
# removed this dependency.
python3-oauthlib
rsync
tar

# disk growing
cloud-utils-growpart

# grub2-efi-x64 ships grub signed for UEFI secure boot. If grub2-efi-x64-modules
# is installed grub will be generated on deployment and unsigned which breaks
# UEFI secure boot.
grub2-efi-x64
efibootmgr
shim-x64
dosfstools
lvm2
mdadm
device-mapper-multipath
iscsi-initiator-utils

dnf-plugins-core

# other packages
net-tools
nfs-utils
openssh-server
rsync
tar
util-linux
wget
python3
tar
bzip2
bc
dracut
dracut-network
rsyslog
hostname
e2fsprogs
ethtool
parted
openssl
dhclient
openssh-clients
bash
vim-minimal
rpm
iputils
perl-interpreter
gawk
xz
squashfs-tools
cpio
sudo
make
bash-completion
nano
pciutils
git
mlocate
sssd
vim-enhanced
systemd-udev
numactl
munge
libevent-devel
tmux
oddjob
oddjob-mkhomedir
redis
unzip
nmap
flex
tk
bison
libgfortran
tcl
gcc-gfortran
libcurl
libnl3-devel
python39
numactl-libs
xfsprogs
zsh
#pkgconf-pkg-config
rpm-build
hwloc
hwloc-libs
hwloc-devel
tcsh
ksh
xorg-x11-fonts-ISO8859-1-75dpi.noarch
xorg-x11-fonts-cyrillic.noarch

# otherpkgs
htop
pmix2
pmix3
pmix4
slurm
slurm-contribs
slurm-libpmi
slurm-pam_slurm
slurm-slurmd
# beeond build dependency
elfutils-libelf-devel

-plymouth
# Remove Intel wireless firmware
-i*-firmware
%end

```

Build the image with:

```shell title="user@local:/cluster-factory-ce/packer-recipes/rocky"
packer build compute.my-cluster.json
```

And send the os image to xcat. [Follow the guide "Build an OS Image with Packer" for more details](/docs/guides/provisioning/packer-build).

Next, you have to configure a service by using a xCAT postscript. Our recommendation is to use a xCAT postscript to pull a Git repository, and based on the content of the repository, copy the files and executes the postscripts in that Git repository.

This way, the GitOps practice is always followed and permits to adapt for the future version of Cluster Factory.

The service:

```properties title="/etc/systemd/system/slurmd.service"
[Unit]
Description=Slurm node daemon
After=network.target munge.service

[Service]
Type=forking
ExecStartPre=/usr/bin/id slurm
Restart=always
RestartSec=3
ExecStart=/usr/sbin/slurmd -d /usr/sbin/slurmstepd --conf-server <node host IP>
ExecReload=/bin/kill -HUP $MAINPID
PIDFile=/var/run/slurmd.pid
KillMode=process
LimitNOFILE=51200
LimitMEMLOCK=infinity
LimitSTACK=infinity

[Install]
WantedBy=multi-user.target
```
