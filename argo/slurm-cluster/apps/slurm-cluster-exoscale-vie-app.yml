apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: slurm-cluster-exoscale-vie-app
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: slurm-cluster
  source:
    repoURL: git@github.com:squarefactory/cluster-factory-ce.git
    targetRevision: HEAD
    path: helm/slurm-cluster
    helm:
      releaseName: slurm-cluster-exoscale-vie

      values: |
        sssd:
          # secret containing sssd.conf
          # Will be mounted in /secrets/sssd
          secretName: sssd-secret

        munge:
          # secret containing munge.key
          # Will be mounted in /secrets/munge
          secretName: munge-secret

        slurmConfig:
          clusterName: exoscale-vie

          compute:
            srunPortRangeStart: 60001
            srunPortRangeEnd: 63000
            debug: debug5

          controller:
            parameters: enable_configless
            debug: debug5

          accounting: |
            AccountingStorageType=accounting_storage/slurmdbd
            AccountingStorageHost=slurmdbd.csquare.gcloud
            AccountingStoragePort=6819
            AccountingStorageTRES=gres/gpu
            AccountingStoreFlags=job_comment,job_env,job_script
            # AccountingStorageEnforce=associations,limits,qos

          defaultResourcesAllocation: |
            DefCpuPerGPU=8
            DefMemPerCpu=600

          scheduling: |
            SchedulerType=sched/backfill
            SelectType=select/cons_tres
            SelectTypeParameters=CR_CPU_Memory
            SchedulerTimeSlice=60
            UnkillableStepTimeout=300

          priorities: |
            # PriorityType=priority/multifactor
            # PriorityFavorSmall=NO
            # PriorityWeightAge=1000
            # PriorityWeightFairshare=10000
            # PriorityWeightTRES=CPU=1000,Mem=2000,GRES/gpu=8000
            # PriorityWeightJobSize=1000
            # PriorityWeightPartition=1000
            # PriorityWeightQOS=1000
            # PriorityDecayHalfLife=0
            # PriorityUsageResetPeriod=MONTHLY

          nodes: |
            NodeName=cn-xxs-[1-10] Feature=cloud State=CLOUD Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=384
            NodeName=cn-xs-[1-10] Feature=cloud State=CLOUD Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=768
            NodeName=cn-s-[1-10] Feature=cloud State=CLOUD Sockets=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=1200
            NodeName=cn-m-[1-10] Feature=cloud State=CLOUD Sockets=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=3000
            NodeName=cn-l-[1-10] Feature=cloud State=CLOUD Sockets=4 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=7000
            NodeName=cn-xl-[1-10] Feature=cloud State=CLOUD Sockets=4 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=14500
            NodeName=cn-xxl-[1-10] Feature=cloud State=CLOUD Sockets=8 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=29500
            NodeName=cn-sgpu2-[1-8] Feature=cloud,cuda State=CLOUD Sockets=12 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=53500 Gres=gpu:1
            NodeName=cn-mgpu2-[1-4] Feature=cloud,cuda State=CLOUD Sockets=16 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=85500 Gres=gpu:2
            NodeName=cn-lgpu2-[1-2] Feature=cloud,cuda State=CLOUD Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=115500 Gres=gpu:3
            NodeName=cn-xlgpu2-1 Feature=cloud,cuda State=CLOUD Sockets=48 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=217500 Gres=gpu:4

          partitions: |
            PartitionName=main Nodes=cn-xxs-[1-10],cn-xs-[1-10],cn-s-[1-10],cn-m-[1-10],cn-l-[1-10],cn-xl-[1-10],cn-xxl-[1-10],cn-sgpu2-[1-8],cn-mgpu2-[1-4],cn-lgpu2-[1-2],cn-xlgpu2-1 Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=xxs Nodes=cn-xxs-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=xs Nodes=cn-xs-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=s Nodes=cn-s-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=m Nodes=cn-m-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=l Nodes=cn-l-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=xl Nodes=cn-xl-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=xxl Nodes=cn-xxl-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=sgpu2 Nodes=cn-sgpu2-[1-8] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=mgpu2 Nodes=cn-mgpu2-[1-4] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=lgpu2 Nodes=cn-lgpu2-[1-2] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO
            PartitionName=xlgpu2 Nodes=cn-xlgpu2-1 Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO

          gres: |
            NodeName=cn-sgpu2-[1-8] File=/dev/nvidia0 AutoDetect=nvml
            NodeName=cn-mgpu2-[1-4] File=/dev/nvidia[0-1] AutoDetect=nvml
            NodeName=cn-lgpu2-[1-2] File=/dev/nvidia[0-2] AutoDetect=nvml
            NodeName=cn-xlgpu2-1 File=/dev/nvidia[0-3] AutoDetect=nvml

          # Extra slurm.conf configuration
          extra: |
            LaunchParameters=enable_nss_slurm
            DebugFlags=Script,Gang,SelectType
            TCPTimeout=5
            CommunicationParameters=NoAddrCache

            # MPI stacks running over Infiniband or OmniPath require the ability to allocate more
            # locked memory than the default limit. Unfortunately, user processes on login nodes
            # may have a small memory limit (check it by ulimit -a) which by default are propagated
            # into Slurm jobs and hence cause fabric errors for MPI.
            PropagateResourceLimitsExcept=MEMLOCK

            ProctrackType=proctrack/cgroup
            TaskPlugin=task/cgroup
            SwitchType=switch/none
            MpiDefault=pmix_v2
            ReturnToService=2 #temp
            GresTypes=gpu
            PreemptType=preempt/qos
            PreemptMode=REQUEUE
            PreemptExemptTime=-1
            Prolog=/etc/slurm/prolog.d/*
            Epilog=/etc/slurm/epilog.d/*

            # Federation
            FederationParameters=fed_display

            #MailProg=/usr/bin/fakemail
            SuspendProgram=/slurm_powersave/suspend.sh
            ResumeProgram=/slurm_powersave/resume.sh
            SuspendTime=180
            ResumeTimeout=600
            MessageTimeout=60

        controller:
          image: 'ghcr.io/squarefactory/slurm-docker:0.1.1-slurmctl-reindeer'
          replicas: 1

          command: ['sh', '-c', 'update-ca-trust && /init']

          imagePullSecrets:
            - name: ghcr-marc-secret

          persistence:
            storageClassName: ''
            accessModes: ['ReadWriteOnce']
            size: 50Gi
            selectorLabels:
              app: slurm-controller
              failure-domain.beta.kubernetes.io/region: at-vie
              failure-domain.beta.kubernetes.io/zone: at-vie-1

          initContainers:
            - name: init-perms-ssh
              image: busybox:1.34.1
              command:
                - sh
                - -c
                - |-
                  cp -RLp /in-ssh/* /out-root-ssh/
                  cp -RLp /in-ssh/* /out-slurm-ssh/
                  chmod 0700 /out-root-ssh/
                  chmod 0600 /out-root-ssh/*
                  chmod 0700 /out-slurm-ssh/
                  chmod 0600 /out-slurm-ssh/*
                  chown -R 1501:1501 /out-slurm-ssh/
              volumeMounts:
                - name: slurmctl-ssh
                  mountPath: /in-ssh
                - name: root-ssh-dir
                  mountPath: /out-root-ssh
                - name: slurm-ssh-dir
                  mountPath: /out-slurm-ssh

          # secret containing jwt_hs256.key
          # Will be mounted in /secrets/slurm
          jwt:
            secretName: slurm-secret

          prologsConfigMap:
          epilogsConfigMap:

          nodeSelector:
            failure-domain.beta.kubernetes.io/region: at-vie
            failure-domain.beta.kubernetes.io/zone: at-vie-1

          resources:
            requests:
              cpu: '250m'
              memory: '256Mi'
            limits:
              cpu:
              memory: '1Gi'

          dnsPolicy: "None"
          dnsConfig:
            nameservers:
              - 172.20.0.10

          # Extra volume mounts
          volumeMounts:
            - name: root-ssh-dir
              mountPath: /root/.ssh
            - name: slurm-ssh-dir
              mountPath: /slurm-ssh-dir
            - name: ca-cert
              mountPath: /etc/pki/ca-trust/source/anchors/csquare.gcloud.ca.pem
              subPath: csquare.gcloud.ca.pem
            - name: slurmctl-exoscale-vie-powersave
              mountPath: /slurm_powersave

          # Extra volumes
          volumes:
            - name: slurmctl-ssh
              secret:
                secretName: slurmctl-ssh-secret
                defaultMode: 384
            - name: ca-cert
              secret:
                secretName: local-ca-secret
            - name: root-ssh-dir
              emptyDir:
                medium: "Memory"
            - name: slurm-ssh-dir
              emptyDir:
                medium: "Memory"
            - name: slurmctl-exoscale-vie-powersave
              configMap:
                name: slurmctl-exoscale-vie-powersave
                defaultMode: 493

          # Extra volume claims
          volumeClaimTemplates: []

          servicePerReplica:
            port: 6817
            annotations:
              metallb.universe.tf/address-pool: at-vie-1-pool
            loadBalancerIP: 172.24.0.10
            type: LoadBalancer

        login:
          enabled: true
          image: 'ghcr.io/squarefactory/slurm-docker:0.1.2-login-reindeer'
          replicas: 2

          command: ['sh', '-c', 'update-ca-trust && /init']

          imagePullSecrets:
            - name: ghcr-marc-secret

          sshd:
            secretName: login-sshd-secret

          nodeSelector:
            failure-domain.beta.kubernetes.io/region: at-vie
            failure-domain.beta.kubernetes.io/zone: at-vie-1

          resources:
            requests:
              cpu: '100m'
              memory: '256Mi'
            limits:
              cpu:
              memory: '1Gi'

          # Extra volume mounts
          volumeMounts:
            - name: ca-cert
              mountPath: /etc/pki/ca-trust/source/anchors/csquare.gcloud.ca.pem
              subPath: csquare.gcloud.ca.pem
            - name: ldap-users-exoscale-vie-pvc
              mountPath: /home/ldap-users
            - name: jobs-logs-exoscale-vie-pvc
              mountPath: /opt/jobs-logs
            - name: images-exoscale-vie-pvc
              mountPath: /opt/images
            - name: experiments-exoscale-vie-pvc
              mountPath: /opt/experiments

          # Extra volumes
          volumes:
            - name: ca-cert
              secret:
                secretName: local-ca-secret
            - name: ldap-users-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: ldap-users-exoscale-vie-pvc
            - name: experiments-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: experiments-exoscale-vie-pvc
            - name: images-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: images-exoscale-vie-pvc
            - name: jobs-logs-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: jobs-logs-exoscale-vie-pvc

          net:
            # Kubernetes host interface
            masterInterface: eth0
            mode: bridge

            # https://www.cni.dev/plugins/current/ipam/static/
            ipam:
              type: host-local
              ranges:
                - - subnet: 172.24.0.0/20
                    rangeStart: 172.24.0.20
                    rangeEnd: 172.24.0.22
                    gateway: 172.24.0.2

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: slurm-cluster

  syncPolicy:
    automated:
      prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).
      selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).
      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).
    syncOptions: []
    retry:
      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0
      backoff:
        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
        factor: 2 # a factor to multiply the base duration after each failed retry
        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy
