apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: slurm-cluster-exoscale-vie-app
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: slurm-cluster
  source:
    repoURL: git@github.com:squarefactory/cluster-factory-ce.git
    targetRevision: HEAD
    path: helm/slurm-cluster
    helm:
      releaseName: slurm-cluster-exoscale-vie

      values: |
        sssd:
          # secret containing sssd.conf
          # Will be mounted in /secrets/sssd
          secretName: sssd-secret

        munge:
          # secret containing munge.key
          # Will be mounted in /secrets/munge
          secretName: munge-secret

        slurmConfig:
          clusterName: exoscale-vie

          compute:
            srunPortRangeStart: 60001
            srunPortRangeEnd: 63000
            debug: debug5

          controller:
            parameters: enable_configless
            debug: debug5

          accounting: |
            AccountingStorageType=accounting_storage/slurmdbd
            AccountingStorageHost=slurmdbd.csquare.gcloud
            AccountingStoragePort=6819
            AccountingStorageTRES=gres/gpu
            AccountingStoreFlags=job_comment,job_env,job_script
            # AccountingStorageEnforce=associations,limits,qos

          defaultResourcesAllocation: |
            DefCpuPerGPU=8
            DefMemPerCpu=600

          scheduling: |
            SchedulerParameters=salloc_wait_nodes,sbatch_wait_nodes,batch_sched_delay=15
            SchedulerType=sched/backfill
            SelectType=select/cons_tres
            SelectTypeParameters=CR_CPU_Memory
            SchedulerTimeSlice=60
            UnkillableStepTimeout=300

          priorities: |
            # PriorityType=priority/multifactor
            # PriorityFavorSmall=NO
            # PriorityWeightAge=1000
            # PriorityWeightFairshare=10000
            # PriorityWeightTRES=CPU=1000,Mem=2000,GRES/gpu=8000
            # PriorityWeightJobSize=1000
            # PriorityWeightPartition=1000
            # PriorityWeightQOS=1000
            # PriorityDecayHalfLife=0
            # PriorityUsageResetPeriod=MONTHLY

          nodes: |
            NodeName=cn-xxs-[1-10] Feature=cloud State=CLOUD CPUS=1 Boards=1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=465 Weight=104
            NodeName=cn-xs-[1-10] Feature=cloud State=CLOUD CPUS=1 Boards=1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=809 Weight=108
            NodeName=cn-s-[1-10] Feature=cloud State=CLOUD CPUS=2 Boards=1 Sockets=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=1817 Weight=218
            NodeName=cn-m-[1-10] Feature=cloud State=CLOUD CPUS=2 Boards=1 Sockets=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=3736 Weight=237
            NodeName=cn-l-[1-10] Feature=cloud State=CLOUD CPUS=4 Boards=1 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=7768 Weight=477
            NodeName=cn-xl-[1-10] Feature=cloud State=CLOUD CPUS=4 Boards=1 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15832 Weight=558
            NodeName=cn-xxl-[1-10] Feature=cloud State=CLOUD CPUS=8 Boards=1 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=31959 Weight=1119
            NodeName=cn-sgpu2-[1-8] Feature=cloud,cuda State=CLOUD CPUS=12 Boards=1 Sockets=2 CoresPerSocket=6 ThreadsPerCore=1 RealMemory=56150 Weight=11761 Gres=gpu:1
            NodeName=cn-mgpu2-[1-4] Feature=cloud,cuda State=CLOUD CPUS=16 Boards=1 Sockets=4 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=90357 Weight=22503 Gres=gpu:2

          partitions: |
            PartitionName=main Nodes=cn-xxs-[1-10],cn-xs-[1-10],cn-s-[1-10],cn-m-[1-10],cn-l-[1-10],cn-xl-[1-10],cn-xxl-[1-10],cn-sgpu2-[1-8],cn-mgpu2-[1-4] Default=YES MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=xxs Nodes=cn-xxs-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=xs Nodes=cn-xs-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=s Nodes=cn-s-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=m Nodes=cn-m-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=l Nodes=cn-l-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=xl Nodes=cn-xl-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=xxl Nodes=cn-xxl-[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=sgpu2 Nodes=cn-sgpu2-[1-8] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE
            PartitionName=mgpu2 Nodes=cn-mgpu2-[1-4] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE

          gres: |
            NodeName=cn-sgpu2-[1-8] Name=gpu File=/dev/nvidia0
            NodeName=cn-mgpu2-[1-4] Name=gpu File=/dev/nvidia[0-1]

          # Extra slurm.conf configuration
          extra: |
            LaunchParameters=enable_nss_slurm
            DebugFlags=Script,Gang,SelectType
            TCPTimeout=5
            CommunicationParameters=NoAddrCache

            # MPI stacks running over Infiniband or OmniPath require the ability to allocate more
            # locked memory than the default limit. Unfortunately, user processes on login nodes
            # may have a small memory limit (check it by ulimit -a) which by default are propagated
            # into Slurm jobs and hence cause fabric errors for MPI.
            PropagateResourceLimitsExcept=MEMLOCK

            ProctrackType=proctrack/cgroup
            TaskPlugin=task/cgroup
            SwitchType=switch/none
            MpiDefault=pmix_v2
            ReturnToService=2 #temp
            GresTypes=gpu
            PreemptType=preempt/qos
            PreemptMode=REQUEUE
            PreemptExemptTime=-1
            Prolog=/etc/slurm/prolog.d/*
            Epilog=/etc/slurm/epilog.d/*

            # Federation
            FederationParameters=fed_display

            MailProg=/usr/bin/fakemail
            SuspendProgram=/slurm_powersave/suspend.sh
            ResumeProgram=/slurm_powersave/resume.sh
            SuspendTime=180
            ResumeTimeout=600
            MessageTimeout=60
            BatchStartTimeout=300

        controller:
          image: 'ghcr.io/squarefactory/slurm-docker:0.1.4-controller-reindeer'
          replicas: 1

          command: ['sh', '-c', 'update-ca-trust && /init']

          imagePullSecrets:
            - name: ghcr-marc-secret

          persistence:
            storageClassName: ''
            accessModes: ['ReadWriteOnce']
            size: 50Gi
            selectorLabels:
              app: slurm-controller
              topology.kubernetes.io/region: at-vie
              topology.kubernetes.io/zone: at-vie-1

          initContainers:
            - name: init-perms-ssh
              image: busybox:1.34.1
              command: ["sh", "-c"]
              args:
                - |-
                  cp -RLp /in-ssh/* /out-root-ssh/;
                  cp -RLp /in-ssh/* /out-slurm-ssh/;
                  chmod 0700 /out-root-ssh/;
                  chmod 0600 /out-root-ssh/*;
                  chmod 0700 /out-slurm-ssh/;
                  chmod 0600 /out-slurm-ssh/*;
                  chown -R 1501:1501 /out-slurm-ssh/;
              volumeMounts:
                - name: slurmctl-ssh
                  mountPath: /in-ssh
                - name: root-ssh-dir
                  mountPath: /out-root-ssh
                - name: slurm-ssh-dir
                  mountPath: /out-slurm-ssh

          # secret containing jwt_hs256.key
          # Will be mounted in /secrets/slurm
          jwt:
            secretName: slurm-secret

          prologsConfigMap: slurmctl-exoscale-vie-prologs
          epilogsConfigMap: slurmctl-exoscale-vie-epilogs

          nodeSelector:
            topology.kubernetes.io/region: at-vie
            topology.kubernetes.io/zone: at-vie-1

          resources:
            requests:
              cpu: '250m'
              memory: '256Mi'
            limits:
              cpu:
              memory: '1Gi'

          dnsPolicy: "None"
          dnsConfig:
            nameservers:
              - 10.96.0.10
            searches:
              - slurm-cluster.svc.cluster.local
              - at1.csquare.run
            options:
              - name: ndots
                value: "0"

          # Extra volume mounts
          volumeMounts:
            - name: root-ssh-dir
              mountPath: /root/.ssh
            - name: slurm-ssh-dir
              mountPath: /slurm-ssh-dir
            - name: ca-cert
              mountPath: /etc/pki/ca-trust/source/anchors/csquare.gcloud.ca.pem
              subPath: csquare.gcloud.ca.pem
            - name: slurmctl-exoscale-vie-powersave
              mountPath: /slurm_powersave/resume.sh
              subPath: resume.sh
            - name: slurmctl-exoscale-vie-powersave
              mountPath: /slurm_powersave/suspend.sh
              subPath: suspend.sh
            - name: fakemail
              mountPath: /usr/bin/fakemail
              subPath: fakemail

          # Extra volumes
          volumes:
            - name: slurmctl-ssh
              secret:
                secretName: slurmctl-ssh-secret
                defaultMode: 384
            - name: ca-cert
              secret:
                secretName: local-ca-secret
            - name: root-ssh-dir
              emptyDir:
                medium: "Memory"
            - name: slurm-ssh-dir
              emptyDir:
                medium: "Memory"
            - name: slurmctl-exoscale-vie-powersave
              configMap:
                name: slurmctl-exoscale-vie-powersave
                defaultMode: 493
            - name: fakemail
              secret:
                secretName: slurmctl-fakemail-secret
                defaultMode: 493

          # Extra volume claims
          volumeClaimTemplates: []

          servicePerReplica:
            port: 6817
            annotations:
              metallb.universe.tf/address-pool: at-vie-1-pool
            type: ClusterIP

        login:
          enabled: true
          image: 'ghcr.io/squarefactory/slurm-docker:0.1.2-login-reindeer'
          replicas: 2

          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxSurge: 1
              maxUnavailable: 1

          command: ['sh', '-c', 'update-ca-trust && /init']

          imagePullSecrets:
            - name: ghcr-marc-secret

          sshd:
            secretName: login-sshd-secret

          nodeSelector:
            topology.kubernetes.io/region: at-vie
            topology.kubernetes.io/zone: at-vie-1

          networks: '[{
            "name": "slurm-cluster-exoscale-vie-login-net"
            }]'

          dnsPolicy: "None"
          dnsConfig:
            nameservers:
              - 10.96.0.10
            searches:
              - slurm-cluster.svc.cluster.local
              - at1.csquare.run
            options:
              - name: ndots
                value: "0"

          resources:
            requests:
              cpu: '100m'
              memory: '256Mi'
            limits:
              cpu:
              memory: '1Gi'

          # Extra volume mounts
          volumeMounts:
            - name: ca-cert
              mountPath: /etc/pki/ca-trust/source/anchors/csquare.gcloud.ca.pem
              subPath: csquare.gcloud.ca.pem
            - name: ldap-users-exoscale-vie-pvc
              mountPath: /home/ldap-users
            - name: jobs-logs-exoscale-vie-pvc
              mountPath: /opt/jobs-logs
            - name: images-exoscale-vie-pvc
              mountPath: /opt/images
            - name: experiments-exoscale-vie-pvc
              mountPath: /opt/experiments

          # Extra volumes
          volumes:
            - name: ca-cert
              secret:
                secretName: local-ca-secret
            - name: ldap-users-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: ldap-users-exoscale-vie-pvc
            - name: experiments-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: experiments-exoscale-vie-pvc
            - name: images-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: images-exoscale-vie-pvc
            - name: jobs-logs-exoscale-vie-pvc
              persistentVolumeClaim:
                claimName: jobs-logs-exoscale-vie-pvc

          net:
            # Kubernetes host interface
            masterInterface: priv0
            mode: l2
            type: ipvlan

            # https://www.cni.dev/plugins/current/ipam/static/
            ipam:
              type: host-local
              ranges:
                - - subnet: 172.24.0.0/20
                    rangeStart: 172.24.0.20
                    rangeEnd: 172.24.0.21
                    gateway: 172.24.0.2
              routes:
                - dst: 10.170.0.0/20 # west1
                - dst: 10.172.0.0/20 # west6
                - dst: 10.252.0.0/14 # west6
                - dst: 10.10.64.0/18 # west6
                - dst: 10.0.0.0/20 # west6
                - dst: 10.10.2.0/24 # Reindeer
                - dst: 10.20.0.0/24 # Custom ranges: Services
                - dst: 10.197.16.0/24 # Custom ranges: Cloud SQL
                - dst: 10.10.0.0/24 # Gondo
                - dst: 172.20.0.0/18 # Exoscale CH-GVA-2
                - dst: 172.24.0.0/20 # Exoscale AT-VIE-1

          rest:
            enabled: true
            image: 'ghcr.io/squarefactory/slurm-docker:0.1.2-rest-reindeer'
            command: ['sh', '-c', 'update-ca-trust && /init']

            # Extra volume mounts
            volumeMounts:
              - name: ca-cert
                mountPath: /etc/pki/ca-trust/source/anchors/csquare.gcloud.ca.pem
                subPath: csquare.gcloud.ca.pem

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: slurm-cluster

  syncPolicy:
    automated:
      prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).
      selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).
      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).
    syncOptions: []
    retry:
      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0
      backoff:
        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
        factor: 2 # a factor to multiply the base duration after each failed retry
        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy
