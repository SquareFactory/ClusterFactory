apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: slurm-cluster-app
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: slurm-cluster
  source:
    repoURL: git@github.com:squarefactory/cluster-factory-ce.git
    targetRevision: HEAD
    path: helm/slurm-cluster
    helm:
      releaseName: slurm-cluster

      values: |
        sssd:
          # secret containing sssd.conf
          # Will be mounted in /secrets/sssd
          secretName: sssd-secret

        munge:
          # secret containing munge.key
          # Will be mounted in /secrets/munge
          secretName: munge-secret

        slurmConfig:
          clusterName: reindeerpizza

          compute:
            srunPortRangeStart: 60001
            srunPortRangeEnd: 61000
            debug: debug5

          accounting: |
            AccountingStorageType=accounting_storage/slurmdbd
            AccountingStorageExternalHost=slurmdbd.csquare.gcloud:6819
            AccountingStorageHost=slurmdbd.csquare.gcloud
            AccountingStoragePort=6819
            AccountingStorageTRES=gres/gpu

          controller:
            ip: 10.10.2.152
            parameters: enable_configless
            debug: debug5

          nodes: |
            NodeName=cn[1-11]  CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128473 Gres=gpu:4
            NodeName=cn12 CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128473 Gres=gpu:4

          partitions: |
            PartitionName=main Nodes=cn[1-11] Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"
            PartitionName=main-beeond Nodes=cn[1-11] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"
            PartitionName=private Nodes=cn12 Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO Hidden=YES AllowQos=admin

          gres: |
            NodeName=cn[1-12] File=/dev/nvidia[0-3] AutoDetect=nvml

          # Extra slurm.conf configuration
          extra: |
            JobCompType=jobcomp/report
            JobCompLoc=https://slurm:f_4UyDHC7%21w%24Cc2KSHFv@slurm.accounting.csquare.gcloud/report
            JobAcctGatherType=jobacct_gather/cgroup
            #MailProg=/usr/bin/fakemail

        controller:
          image: 'ghcr.io/squarefactory/slurm-docker:0.1.1-slurmctl-reindeer'
          replicas: 1

          command: ['sh', '-c', 'update-ca-trust && /init']

          imagePullSecrets:
            - name: ghcr-marc-secret

          persistence:
            storageClassName: ''
            accessModes: ['ReadWriteOnce']
            size: 50Gi
            selectorLabels:
              app: slurm-controller

          initContainers:
            - name: init-perms-ssh
              image: busybox:1.34.1
              command:
                - sh
                - -c
                - |-
                  cp -RLp /in-ssh/* /out-ssh/
                  chmod -R 0600 /out-ssh/
              volumeMounts:
                - name: slurmctl-ssh
                  mountPath: /in-ssh
                - name: root-ssh-dir
                  mountPath: /out-ssh

          # secret containing jwt_hs256.key
          # Will be mounted in /secrets/slurm
          jwt:
            secretName: slurm-secret

          prologsConfigMap: 'slurmctl-prologs'
          epilogsConfigMap: 'slurmctl-epilogs'

          # Extra volume mounts
          volumeMounts:
            - name: root-ssh-dir
              mountPath: /root/.ssh
            - name: ca-cert
              mountPath: /etc/pki/ca-trust/source/anchors/csquare.gcloud.ca.pem
              subPath: csquare.gcloud.ca.pem

          # Extra volumes
          volumes:
            - name: slurmctl-ssh
              secret:
                secretName: slurmctl-ssh-secret
                defaultMode: 384
            - name: ca-cert
              secret:
                secretName: local-ca-secret
            - name: root-ssh-dir
              emptyDir:
                medium: "Memory"

          # Extra volume claims
          volumeClaimTemplates: []

          servicePerReplica:
            port: 6817
            loadBalancerIP: 10.10.2.152
            type: LoadBalancer

        login:
          enabled: true
          image: 'ghcr.io/squarefactory/slurm-docker:0.1.2-login-reindeer'
          replicas: 1

          command: ['sh', '-c', 'update-ca-trust && /init']

          imagePullSecrets:
            - name: ghcr-marc-secret

          sshd:
            secretName: login-sshd-secret

          # Extra volume mounts
          volumeMounts:
            - name: ca-cert
              mountPath: /etc/pki/ca-trust/source/anchors/csquare.gcloud.ca.pem
              subPath: csquare.gcloud.ca.pem
            - name: slurm-login-profile
              mountPath: /etc/profile.d/z00_lmod.sh
              subPath: z00_lmod.sh
            - name: slurm-login-profile
              mountPath: /etc/profile.d/z01_lmod.sh
              subPath: z01_lmod.sh
            - name: ldap-users-pvc
              mountPath: /home/ldap-users
            - name: jobs-logs-pvc
              mountPath: /opt/jobs-logs
            - name: images-pvc
              mountPath: /opt/images
            - name: experiments-pvc
              mountPath: /opt/experiments
            - name: software-sion-csquare-run-pvc
              mountPath: /opt/software
            - name: stdenv-sion-csquare-run-pvc
              mountPath: /opt/stdenv
            - name: unpacked-sion-csquare-run-pvc
              mountPath: /opt/unpacked

          # Extra volumes
          volumes:
            - name: ca-cert
              secret:
                secretName: local-ca-secret
            - name: slurm-login-profile
              configMap:
                name: slurm-login-profile
                defaultMode: 493

          # Extra volume claims
          volumeClaimTemplates:
            - metadata:
                name: ldap-users-pvc
              spec:
                volumeName: ldap-users-pv
                accessModes: [ReadWriteMany]
                storageClassName: ''
                resources:
                  requests:
                    storage: 1000Gi
            - metadata:
                name: jobs-logs-pvc
              spec:
                volumeName: jobs-logs-pv
                accessModes: [ReadWriteMany]
                storageClassName: ''
                resources:
                  requests:
                    storage: 1000Gi
            - metadata:
                name: images-pvc
              spec:
                volumeName: images-pv
                accessModes: [ReadWriteMany]
                storageClassName: ''
                resources:
                  requests:
                    storage: 1000Gi
            - metadata:
                name: experiments-pvc
              spec:
                volumeName: experiments-pv
                accessModes: [ReadWriteMany]
                storageClassName: ''
                resources:
                  requests:
                    storage: 1000Gi
            - metadata:
                name: software-sion-csquare-run-pvc
              spec:
                accessModes: [ReadOnlyMany]
                storageClassName: 'software-sion-csquare-run'
                resources:
                  requests:
                    storage: 1Gi
            - metadata:
                name: unpacked-sion-csquare-run-pvc
              spec:
                accessModes: [ReadOnlyMany]
                storageClassName: 'unpacked-sion-csquare-run'
                resources:
                  requests:
                    storage: 1Gi
            - metadata:
                name: stdenv-sion-csquare-run-pvc
              spec:
                accessModes: [ReadOnlyMany]
                storageClassName: 'stdenv-sion-csquare-run'
                resources:
                  requests:
                    storage: 1Gi

          servicePerReplica:
            enabled: true
            annotations:
              metallb.universe.tf/address-pool: slurm-ips
            loadBalancerSourceRanges: [10.10.2.40/27]
            type: LoadBalancer

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: slurm-cluster

  syncPolicy:
    automated:
      prune: false # Specifies if resources should be pruned during auto-syncing ( false by default ).
      selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).
      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).
    syncOptions: []
    retry:
      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0
      backoff:
        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
        factor: 2 # a factor to multiply the base duration after each failed retry
        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy
